[
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "",
    "text": "R for Data Science Online Book, Chapters 10, 11, 12, 7\n\nApplied predictive Modelling, 3.3 (only transformations to resolve outliers), 3.4",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.3 Importing, Quality Check and Cleansing"
    ]
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#tibbles",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#tibbles",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Tibbles",
    "text": "Tibbles\n\nTibbles\n\nWe work with “tibbles” instead of R’s traditional data.frame in the tidyverse environment.\nThe tibble package, which provides opinionated data frames that make working in the tidyverse a little easier.\nTry vignette(\"tibble\") for more information.\n\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.4\n✔ tidyr   1.3.0     ✔ stringr 1.5.1\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nCreating Tibbles\n\nCoerce a data frame to a tibble: as_tibble()\ntibble()\nHave non-syntactic names with backticks `\ntribble(): transposed tibble\n\n\n\nExample 1\n\nas_tibble(iris)\n\n\n  \n\n\ntibble(\n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y\n)\n\n\n  \n\n\n\n\n\nExample 2\n\ntibble(\n  `:)` = \"smile\", \n  ` ` = \"space\",\n  `2000` = \"number\"\n)\n\n\n  \n\n\ntribble(\n  ~x, ~y, ~z,\n  #--|--|----\n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)\n\n\n  \n\n\n\n\n\nPipe data using %&gt;%\n\nUse %&gt;% to emphasise a sequence of actions, rather than the object that the actions are being performed on\npronounce %&gt;% when reading code “then”\nNo need to name things\n%&gt;% should always have a space before it, and should usually be followed by a new line.\n\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    Sepal.Length = mean(Sepal.Length),\n    Sepal.Width = mean(Sepal.Width),\n    Species = n_distinct(Species)\n  )\n\n\n  \n\n\n\n\n\nTibble v.s. data.frame\n\ntibble() does much less:\nit never changes the type of the inputs (e.g. it never converts strings to factors!),\nit never changes the names of variables, and\nit never creates row names.\nPrinting\n\nTibbles show only the first 10 rows\neach column reports its type\nuse print() to display more rows (n) and columns (width)\n\nSubsetting\n\npull out a single variable: $ (extract by name) and [[ ]] (extract by name or position)\nin a pipe %&gt;%, use the special placeholder .\n\n\n\n\nExamples\n\ndf &lt;- tibble(\n  x = runif(5),\n  y = rnorm(5)\n)\n# Extract by name\ndf$x\n\n[1] 0.73516989 0.03066636 0.03909845 0.69382542 0.60253210\n\ndf[[\"x\"]]\n\n[1] 0.73516989 0.03066636 0.03909845 0.69382542 0.60253210\n\n# Extract by position\ndf[[1]]\n\n[1] 0.73516989 0.03066636 0.03909845 0.69382542 0.60253210\n\ndf %&gt;% .$x\n\n[1] 0.73516989 0.03066636 0.03909845 0.69382542 0.60253210\n\ndf %&gt;% .[[\"x\"]]\n\n[1] 0.73516989 0.03066636 0.03909845 0.69382542 0.60253210",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.3 Importing, Quality Check and Cleansing"
    ]
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#import-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#import-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Import Data",
    "text": "Import Data\n\nImport Data\n\nRead The Art of Data Science, Chapter 5\nRead R for Data Science, Chapter 11",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.3 Importing, Quality Check and Cleansing"
    ]
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#tidy-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#tidy-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data\n\nThe same data can be organised in different ways.\nThe tidy data is easy to work with.\nThere are three interrelated rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nPractical instructions:\n\nPut each dataset in a tibble.\nPut each variable in a column.\n\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data\n\n\nlibrary(tidyverse)\n\n\n\nPivoting: Longer\n\nA common problem is a dataset where some of the column names are not names of variables, but values of a variable.\nExample: table4a: the column names 1999 and 2000 represent values of the year variable, the values in the 1999 and 2000 columns represent values of the cases variable, and each row represents two observations, not one.\n\n\n#table4a\n\ntable4a %&gt;% \n  pivot_longer(cols = c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n\n  \n\n\n\n\n\nPivoting: Wider\n\npivot_wider() is the opposite of pivot_longer().\nYou use it when an observation is scattered across multiple rows.\nFor example, take table2: an observation is a country in a year, but each observation is spread across two rows.\n\n\ntable2 %&gt;%\n    pivot_wider(names_from = type, values_from = count)\n\n\n  \n\n\n\n\n\nSeparating\n\ntable3 has a different problem: we have one column (rate) that contains two variables (cases and population).\nseparate() pulls apart one column into multiple columns, by splitting wherever a separator character appears.\nBy default, separate() will split values wherever it sees a non-alphanumeric character (i.e. a character that isn’t a number or letter).\nUse sep argument to use a specific character to separate a column\n\n::: {.cell}\ntable3\n::: {.cell-output-display}\n\n  \n\n::: :::\n\n\nExample: Separating\n\ntable3 %&gt;% \n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\n\n  \n\n\n#table3 %&gt;% \n#  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\", convert=TRUE)\n\n#table3 %&gt;% \n#  separate(year, into = c(\"century\", \"year\"), sep = 2)\n\n\n\nUnite\n\nunite() is the inverse of separate(): it combines multiple columns into a single column.\nThe default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use ““.\n\n\ntable5 %&gt;% \n  unite(new, century, year)\n\n\n  \n\n\n#table5 %&gt;% \n#  unite(new, century, year, sep = \"\")\n\n\n\nMissing Value\n\n\nA value can be missing in one of two possible ways:\n\nExplicitly, i.e. flagged with NA. – the presence of an absence\nImplicitly, i.e. simply not present in the data. – the absence of a presence\n\n\n\n  stocks &lt;- tibble(\n  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),\n  qtr    = c(   1,    2,    3,    4,    2,    3,    4),\n  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\n\n\n\nMaking implicit missing values explicit\n\nusing pivot_wider\nusing complete()\n\n\nstocks %&gt;% \n  pivot_wider(names_from = year, values_from = return)\n\n\n  \n\n\n#stocks %&gt;% \n#  complete(year, qtr)\n\n\n\nMaking explicit missing values implicit\n\nSet values_drop_na = TRUE in pivot_longer() to turn explicit missing values implicit\n\n\nstocks %&gt;% \n  pivot_wider(names_from = year, values_from = return) %&gt;% \n  pivot_longer(\n    cols = c(`2015`, `2016`), \n    names_to = \"year\", \n    values_to = \"return\", \n    values_drop_na = TRUE\n  )\n\n\n  \n\n\n\n\n\nFill the missing values with fill()\n\nfill() takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward).\n\n\ntreatment &lt;- tribble(\n  ~ person,           ~ treatment, ~response,\n  \"Derrick Whitmore\", 1,           7,\n  NA,                 2,           10,\n  NA,                 3,           9,\n  \"Katherine Burke\",  1,           4\n)\n\ntreatment %&gt;% \n  fill(person)",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.3 Importing, Quality Check and Cleansing"
    ]
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#assessing-data-quality",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#assessing-data-quality",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Assessing data quality",
    "text": "Assessing data quality\n\nData issues\n\nMissing data\nIrregular data and outliers\nUninformative data\nCensored and truncated data\nHigh cardinality features\nImbalanced data\n\n\n\nDiagnosing missing data\n\nDescriptive statistics\nPlots\n and \n\n\n\nMissing data\n\nUnderstand why the values are missing\nStructurally missing or not\n\ne.g. the number of children a man has given birth to\n\nInformative missingness: the pattern of missing data is related to outcome\n\ne.g. in a drug study, the side effect so bad that the patients drop out\n\n\n\n\nHandling missing values\n\ninclude a missing indicator (dummy variable)\n\nif the pattern of missingness is informative\n\nsome models can account for missing data, such as tree-based techniques\nmany models cannot tolerate missing values\n\nlinear models, neural networks, SVMs\n\nremove the observations or variables as a last resort\n\nmay be feasible for large dataset\n\nimpute missing values\n\n\n\nImputing missing values\n\nuse the information in the training set predictors to estimate the values of other predictors\n\nvia mean, median or mode\nvia model-based, such as K-nearest neighbor model\n\nextensively studied in the statistical literature in terms inference; not a big concern for predictive modelling\n\n\n\nIrregular data/Outliers\n\nDetection\n\ndescriptive statistics\nplots, such as boxplot, scatter plot\noutlier detection models\n\nHandling\n\ndata validation, make sure no recording errors\nremove or change values\noutliers might belong to a different population than the other samples\nmodels resistant to outliers, e.g. tree-based classification methods, SVM\ntransformations to minimise the problem using spatial sign – each sample is divided by its squared norm.\n\n\nExample: \n\n\nUninformative data\n\nrepetitive\nduplicates\nirrelevant\ncollinearity: a pair of predictor variables have a substantial correlation with each other\n\nredundant predictors add more complexity\nresults in highly unstable models, numerical errors and degraded predictive performance\nFor linear regression, models such as variance inflation factor (VIF) can be used to identify predictors that are impacted\n\n\n\n\nCensored data\n\nThe value of an observation is only partially known\nFor interpretation or inference\n\nusually treated in a formal manner by making assumptions about the censoring mechanism\n\nFor prediction\n\nusually treated as missing data or use the censored value as observed value\n\n\nExamples: general insurance (policy limits); life insurance (age groups of mortality data)\n\n\nHigh cardinality features\n\nCategorical predictors with many unique factor levels\nHigh cardinality features (eg. post codes, medical condition coding or similar)\n\nSome references for dealing with high cardinality features:\n\nDealing with features that have high cardinality\nEncoding High-Cardinality String Categorical Variables\nSimilarity Encoding for Learning with Dirty Categorical Variables\nNonlife Insurance Risk Classification Using Categorical Embedding\nUsing Random Effects to Account for High-Cardinality Categorical Features and Repeated Measures in Deep Neural Networks\n\n\n\nImbalanced data\n\nImbalance between control and treatment observations can cause modelling problems\nConstruct a balanced training set to improve modelling outcomes for imbalanced data\n\nUndersampling: reduce the number of patterns within the majority class data set to make it equivalent to other classes\nOversampling: generate more data within the minority class\n\n\n\n\nData validation\n\nValidate data against other sources and the same data from previous runs\nTalk to people who input and use the data to assess data quality",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.3 Importing, Quality Check and Cleansing"
    ]
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#exploratory-data-analysis-eda",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.html#exploratory-data-analysis-eda",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nExploratory Data Analysis\n\nGenerate questions about your data.\nSearch for answers by visualising, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\nWe’ll combine dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions.\nTwo types of questions will always be useful for making discoveries within your data:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?\n\n\n\nlibrary(tidyverse)\n\n\n\nVariation\n\nVariation is the tendency of the values of a variable to change from measurement to measurement.\nVariables:\n\ncontinuous variable: if it can take any of an infinite set of ordered values\ncategorical variable: if it can only take one of a small set of values.\n\n\n\n\nVisualising Distributios: Categorical Variable\n\nTo examine the distribution of a categorical variable, use a bar chart.\nData: diamonds. Check ?diamonds for more information of the dataset. \n\n\nggplot(data = diamonds) +\n  geom_bar(mapping = aes(x = cut))\n\n\n\n\n\n\n\n#diamonds %&gt;% \n#  count(cut)\n\n\n\nVisualising Distributions: Continuous Variable\n\nTo examine the distribution of a continuous variable, use a histogram.\n\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)\n\n\n\n\n\n\n\n#diamonds %&gt;% \n#  count(cut_width(carat, 0.5))\n\n\n\nExercise 1\n\nPlot the histogram of the diamonds with a size of less than 3 carats (using filter) and choose a smaller binwidth of 0.1. \n\n\nsmaller &lt;- diamonds %&gt;% \n  filter(carat &lt; 3)\n  \nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\n\n\n\nExericese 2\n\nOverlay multiple histograms in the same plot by cut using geom_freqpoly() \n\n \n\nggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +\n  geom_freqpoly(binwidth = 0.1)\n\n\n\n\n\n\n\n\n\n\n\nTypical Values\n\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data.\nTo turn this information into useful questions, look for anything unexpected:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?\n\n\n\n\nExample\n\nLook at the histogram below, what questions can you ask?\n\n\nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\n\n\n\nExample: Solution\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\nWhy are there no diamonds bigger than 3 carats?\n\n\n\nUnusual Values (Outliers)\n\n\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern.\nSometimes outliers are data entry errors; other times outliers suggest important new science.\nWhen you have a lot of data, outliers are sometimes difficult to see in a histogram.\n\n\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = y), binwidth = 0.5)\n\n\n\n\n\n\n\n\n\n\nVisualising Outliers\n\nTo make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian() and ylim() or xlim() to zoom into the y-axis or x-axis.\n\n\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))\n\n\n\n\n\n\n\n\n\n\nDisplay all the unusual values\n\nWe pluck them out with dplyr.\nWhat questions you may have?\n\n\nunusual &lt;- diamonds %&gt;% \n  filter(y &lt; 3 | y &gt; 20) %&gt;% \n  select(price, x, y, z) %&gt;%\n  arrange(y)\nunusual\n\n\n  \n\n\n\n\n\nDeal with Outliers: Example\n\nIn the ‘Diamond’ example, the y variable measures one of the three dimensions of these diamonds, in mm.\nWe know that diamonds can’t have a width of 0mm, so these values must be incorrect.\nWe might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars!\n\n\n\nDeal with Outliers\n\nRepeat your analysis with and without the outliers.\nIf they have minimal effect on the results and you can’t figure out why they’re there, it’s reasonable to replace them with missing values and move on.\nIf they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.\n\n\n\nMissing Values\n\n\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n\nDrop the entire row with the strange values (not recommend, why?)\n\n\n\ndiamonds2 &lt;- diamonds %&gt;% \n  filter(between(y, 3, 20))\n#diamonds2 \n\n\nReplace the unusual values with missing values (NA) using mutate() with ifelse() or case_when()\n\n\ndiamonds2 &lt;- diamonds %&gt;% \n  mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y))\n#diamonds2\n\n\nggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed.\n\n\n\nCovariation\n\nIf variation describes the behavior within a variable, covariation describes the behavior between variables.\nCovariation is the tendency for the values of two or more variables to vary together in a related way.\nThe best way to spot covariation is to visualise the relationship between two or more variables.\nWe consider three different combinations\n\nA categorical and continuous variable\nTwo categorical variables\nTwo continuous variables\n\n\n\n\nA Categorical and Continuous Variable\n\n\nExplore the distribution of a continuous variable broken down by a categorical variable\nusing geom_freqpoly(), for example see \nIt’s hard to see the difference in distribution because the overall counts differ so much, see .\nInstead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one.\n\n\nggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + \n  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\nAnother alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot.\n\n\n\n\nBoxplot, source: R for Data Science\n\n\n\n\nExample 1\n\nTake a look at the distribution of diamond price by cut. What can you find?\n\n\nggplot(data = diamonds, mapping = aes(x = cut, y = price)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nIt supports the counterintuitive finding that better quality diamonds are cheaper on average! Why?\n\n\n\nExample 2\n\nLook at the mpg dataset. We are interested to know how highway mileage (hwy) varies across classes (class)\nTo make the trend easier to see, we can reorder (reorder) class based on the median value (FUN=median) of hwy.\nIf you have long variable names, geom_boxplot() will work better if you flip it 90°. You can do that with coord_flip()\n\n\nggplot(data = mpg) +\n  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) #+\n\n\n\n\n\n\n\n#  coord_flip()\n\n\n\nTwo Categorical Variables\n\nTo visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. One way to do that is to rely on the built-in geom_count()\n\n\nggplot(data = diamonds) +\n  geom_count(mapping = aes(x = cut, y = color))\n\n\n\n\n\n\n\n\n\n\nExample\n\nAnother approach is to compute the count with dplyr.\n\n\ndiamonds %&gt;% \n  count(color, cut)\n\n\n  \n\n\n\n\n\nExample\n\nThen visualise with geom_tile() and the fill aesthetic.\n\n\ndiamonds %&gt;% \n  count(color, cut) %&gt;%  \n  ggplot(mapping = aes(x = color, y = cut)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\n\n\n\n\n\nTwo Continuous Variables\n\nOne great way to visualise the covariation between two continuous variables is to draw a scatter plot with geom_point(). You can see covariation as a pattern in the points.\nExample: visualise the relationship between the carat size and price of a diamond.\n\n\nggplot(data = diamonds) +\n  geom_point(mapping = aes(x = carat, y = price))\n\n\n\n\n\n\n\n\n\n\nOther ways to visualisize the relationship\n\nuse the alpha aesthetic to add transparency\nuse geom_bin2d() and geom_hex() to bin in two dimensions\nbin one continuous variable so it acts like a categorical variable\n\n\n\nExample: add transparency\n\nggplot(data = diamonds) + \n  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)\n\n\n\n\n\n\n\n\n\n\nFrom Data Patterns to Models\n\nPatterns in your data provide clues about relationships\nModels are a tool for extracting patterns out of data",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.3 Importing, Quality Check and Cleansing"
    ]
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#reading-list",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#reading-list",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Reading List",
    "text": "Reading List\n\nR for Data Science Online Book, Chapters 10, 11, 12, 7\n\nApplied predictive Modelling, 3.3 (only transformations to resolve outliers), 3.4"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#tibbles-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#tibbles-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Tibbles",
    "text": "Tibbles\n\nWe work with “tibbles” instead of R’s traditional data.frame in the tidyverse environment.\nThe tibble package, which provides opinionated data frames that make working in the tidyverse a little easier.\nTry vignette(\"tibble\") for more information.\n\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#creating-tibbles",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#creating-tibbles",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Creating Tibbles",
    "text": "Creating Tibbles\n\nCoerce a data frame to a tibble: as_tibble()\ntibble()\nHave non-syntactic names with backticks `\ntribble(): transposed tibble"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example 1",
    "text": "Example 1\n\nas_tibble(iris)\n\n\n  \n\n\ntibble(\n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y\n)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-2",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-2",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example 2",
    "text": "Example 2\n\ntibble(\n  `:)` = \"smile\", \n  ` ` = \"space\",\n  `2000` = \"number\"\n)\n\n\n  \n\n\ntribble(\n  ~x, ~y, ~z,\n  #--|--|----\n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#pipe-data-using",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#pipe-data-using",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Pipe data using %>%",
    "text": "Pipe data using %&gt;%\n\nUse %&gt;% to emphasise a sequence of actions, rather than the object that the actions are being performed on\npronounce %&gt;% when reading code “then”\nNo need to name things\n%&gt;% should always have a space before it, and should usually be followed by a new line.\n\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    Sepal.Length = mean(Sepal.Length),\n    Sepal.Width = mean(Sepal.Width),\n    Species = n_distinct(Species)\n  )"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#tibble-v.s.-data.frame",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#tibble-v.s.-data.frame",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Tibble v.s. data.frame",
    "text": "Tibble v.s. data.frame\n\ntibble() does much less:\nit never changes the type of the inputs (e.g. it never converts strings to factors!),\nit never changes the names of variables, and\nit never creates row names.\nPrinting\n\nTibbles show only the first 10 rows\neach column reports its type\nuse print() to display more rows (n) and columns (width)\n\nSubsetting\n\npull out a single variable: $ (extract by name) and [[ ]] (extract by name or position)\nin a pipe %&gt;%, use the special placeholder ."
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#examples",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#examples",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Examples",
    "text": "Examples\n\ndf &lt;- tibble(\n  x = runif(5),\n  y = rnorm(5)\n)\n# Extract by name\ndf$x\n\n[1] 0.5603771 0.7288496 0.4594416 0.3568941 0.5793466\n\ndf[[\"x\"]]\n\n[1] 0.5603771 0.7288496 0.4594416 0.3568941 0.5793466\n\n# Extract by position\ndf[[1]]\n\n[1] 0.5603771 0.7288496 0.4594416 0.3568941 0.5793466\n\ndf %&gt;% .$x\n\n[1] 0.5603771 0.7288496 0.4594416 0.3568941 0.5793466\n\ndf %&gt;% .[[\"x\"]]\n\n[1] 0.5603771 0.7288496 0.4594416 0.3568941 0.5793466"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#import-data-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#import-data-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Import Data",
    "text": "Import Data\n\nRead The Art of Data Science, Chapter 5\nRead R for Data Science, Chapter 11"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#tidy-data-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#tidy-data-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nThe same data can be organised in different ways.\nThe tidy data is easy to work with.\nThere are three interrelated rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nPractical instructions:\n\nPut each dataset in a tibble.\nPut each variable in a column.\n\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#pivoting-longer",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#pivoting-longer",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Pivoting: Longer",
    "text": "Pivoting: Longer\n\nA common problem is a dataset where some of the column names are not names of variables, but values of a variable.\nExample: table4a: the column names 1999 and 2000 represent values of the year variable, the values in the 1999 and 2000 columns represent values of the cases variable, and each row represents two observations, not one.\n\n\n#table4a\n\ntable4a %&gt;% \n  pivot_longer(cols = c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#pivoting-wider",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#pivoting-wider",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Pivoting: Wider",
    "text": "Pivoting: Wider\n\npivot_wider() is the opposite of pivot_longer().\nYou use it when an observation is scattered across multiple rows.\nFor example, take table2: an observation is a country in a year, but each observation is spread across two rows.\n\n\ntable2 %&gt;%\n    pivot_wider(names_from = type, values_from = count)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#separating",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#separating",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Separating",
    "text": "Separating\n\ntable3 has a different problem: we have one column (rate) that contains two variables (cases and population).\nseparate() pulls apart one column into multiple columns, by splitting wherever a separator character appears.\nBy default, separate() will split values wherever it sees a non-alphanumeric character (i.e. a character that isn’t a number or letter).\nUse sep argument to use a specific character to separate a column\n\n::: {.cell}\ntable3\n::: {.cell-output-display}\n\n  \n\n::: :::"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-separating",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-separating",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example: Separating",
    "text": "Example: Separating\n\ntable3 %&gt;% \n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\n\n  \n\n\n#table3 %&gt;% \n#  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\", convert=TRUE)\n\n#table3 %&gt;% \n#  separate(year, into = c(\"century\", \"year\"), sep = 2)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#unite",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#unite",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Unite",
    "text": "Unite\n\nunite() is the inverse of separate(): it combines multiple columns into a single column.\nThe default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use ““.\n\n\ntable5 %&gt;% \n  unite(new, century, year)\n\n\n  \n\n\n#table5 %&gt;% \n#  unite(new, century, year, sep = \"\")"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#missing-value",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#missing-value",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Missing Value",
    "text": "Missing Value\n\n\nA value can be missing in one of two possible ways:\n\nExplicitly, i.e. flagged with NA. – the presence of an absence\nImplicitly, i.e. simply not present in the data. – the absence of a presence\n\n\n\n  stocks &lt;- tibble(\n  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),\n  qtr    = c(   1,    2,    3,    4,    2,    3,    4),\n  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#making-implicit-missing-values-explicit",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#making-implicit-missing-values-explicit",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Making implicit missing values explicit",
    "text": "Making implicit missing values explicit\n\nusing pivot_wider\nusing complete()\n\n\nstocks %&gt;% \n  pivot_wider(names_from = year, values_from = return)\n\n\n  \n\n\n#stocks %&gt;% \n#  complete(year, qtr)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#making-explicit-missing-values-implicit",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#making-explicit-missing-values-implicit",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Making explicit missing values implicit",
    "text": "Making explicit missing values implicit\n\nSet values_drop_na = TRUE in pivot_longer() to turn explicit missing values implicit\n\n\nstocks %&gt;% \n  pivot_wider(names_from = year, values_from = return) %&gt;% \n  pivot_longer(\n    cols = c(`2015`, `2016`), \n    names_to = \"year\", \n    values_to = \"return\", \n    values_drop_na = TRUE\n  )"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#fill-the-missing-values-with-fill",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#fill-the-missing-values-with-fill",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Fill the missing values with fill()",
    "text": "Fill the missing values with fill()\n\nfill() takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward).\n\n\ntreatment &lt;- tribble(\n  ~ person,           ~ treatment, ~response,\n  \"Derrick Whitmore\", 1,           7,\n  NA,                 2,           10,\n  NA,                 3,           9,\n  \"Katherine Burke\",  1,           4\n)\n\ntreatment %&gt;% \n  fill(person)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#data-issues",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#data-issues",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Data issues",
    "text": "Data issues\n\nMissing data\nIrregular data and outliers\nUninformative data\nCensored and truncated data\nHigh cardinality features\nImbalanced data"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#diagnosing-missing-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#diagnosing-missing-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Diagnosing missing data",
    "text": "Diagnosing missing data\n\nDescriptive statistics\nPlots\n and"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#missing-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#missing-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Missing data",
    "text": "Missing data\n\nUnderstand why the values are missing\nStructurally missing or not\n\ne.g. the number of children a man has given birth to\n\nInformative missingness: the pattern of missing data is related to outcome\n\ne.g. in a drug study, the side effect so bad that the patients drop out"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#handling-missing-values",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#handling-missing-values",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Handling missing values",
    "text": "Handling missing values\n\ninclude a missing indicator (dummy variable)\n\nif the pattern of missingness is informative\n\nsome models can account for missing data, such as tree-based techniques\nmany models cannot tolerate missing values\n\nlinear models, neural networks, SVMs\n\nremove the observations or variables as a last resort\n\nmay be feasible for large dataset\n\nimpute missing values"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#imputing-missing-values",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#imputing-missing-values",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Imputing missing values",
    "text": "Imputing missing values\n\nuse the information in the training set predictors to estimate the values of other predictors\n\nvia mean, median or mode\nvia model-based, such as K-nearest neighbor model\n\nextensively studied in the statistical literature in terms inference; not a big concern for predictive modelling"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#irregular-dataoutliers",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#irregular-dataoutliers",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Irregular data/Outliers",
    "text": "Irregular data/Outliers\n\nDetection\n\ndescriptive statistics\nplots, such as boxplot, scatter plot\noutlier detection models\n\nHandling\n\ndata validation, make sure no recording errors\nremove or change values\noutliers might belong to a different population than the other samples\nmodels resistant to outliers, e.g. tree-based classification methods, SVM\ntransformations to minimise the problem using spatial sign – each sample is divided by its squared norm.\n\n\nExample:"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#uninformative-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#uninformative-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Uninformative data",
    "text": "Uninformative data\n\nrepetitive\nduplicates\nirrelevant\ncollinearity: a pair of predictor variables have a substantial correlation with each other\n\nredundant predictors add more complexity\nresults in highly unstable models, numerical errors and degraded predictive performance\nFor linear regression, models such as variance inflation factor (VIF) can be used to identify predictors that are impacted"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#censored-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#censored-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Censored data",
    "text": "Censored data\n\nThe value of an observation is only partially known\nFor interpretation or inference\n\nusually treated in a formal manner by making assumptions about the censoring mechanism\n\nFor prediction\n\nusually treated as missing data or use the censored value as observed value\n\n\nExamples: general insurance (policy limits); life insurance (age groups of mortality data)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#high-cardinality-features",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#high-cardinality-features",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "High cardinality features",
    "text": "High cardinality features\n\nCategorical predictors with many unique factor levels\nHigh cardinality features (eg. post codes, medical condition coding or similar)\n\nSome references for dealing with high cardinality features:\n\nDealing with features that have high cardinality\nEncoding High-Cardinality String Categorical Variables\nSimilarity Encoding for Learning with Dirty Categorical Variables\nNonlife Insurance Risk Classification Using Categorical Embedding\nUsing Random Effects to Account for High-Cardinality Categorical Features and Repeated Measures in Deep Neural Networks"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#imbalanced-data",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#imbalanced-data",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Imbalanced data",
    "text": "Imbalanced data\n\nImbalance between control and treatment observations can cause modelling problems\nConstruct a balanced training set to improve modelling outcomes for imbalanced data\n\nUndersampling: reduce the number of patterns within the majority class data set to make it equivalent to other classes\nOversampling: generate more data within the minority class"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#data-validation",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#data-validation",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Data validation",
    "text": "Data validation\n\nValidate data against other sources and the same data from previous runs\nTalk to people who input and use the data to assess data quality"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#exploratory-data-analysis",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#exploratory-data-analysis",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nGenerate questions about your data.\nSearch for answers by visualising, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\nWe’ll combine dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions.\nTwo types of questions will always be useful for making discoveries within your data:\n\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?\n\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#variation",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#variation",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Variation",
    "text": "Variation\n\nVariation is the tendency of the values of a variable to change from measurement to measurement.\nVariables:\n\ncontinuous variable: if it can take any of an infinite set of ordered values\ncategorical variable: if it can only take one of a small set of values."
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#visualising-distributios-categorical-variable",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#visualising-distributios-categorical-variable",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Visualising Distributios: Categorical Variable",
    "text": "Visualising Distributios: Categorical Variable\n\nTo examine the distribution of a categorical variable, use a bar chart.\nData: diamonds. Check ?diamonds for more information of the dataset. \n\n\nggplot(data = diamonds) +\n  geom_bar(mapping = aes(x = cut))\n\n#diamonds %&gt;% \n#  count(cut)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#visualising-distributions-continuous-variable",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#visualising-distributions-continuous-variable",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Visualising Distributions: Continuous Variable",
    "text": "Visualising Distributions: Continuous Variable\n\nTo examine the distribution of a continuous variable, use a histogram.\n\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)\n\n#diamonds %&gt;% \n#  count(cut_width(carat, 0.5))"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#exercise-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#exercise-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nPlot the histogram of the diamonds with a size of less than 3 carats (using filter) and choose a smaller binwidth of 0.1. \n\n\nsmaller &lt;- diamonds %&gt;% \n  filter(carat &lt; 3)\n  \nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#exericese-2",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#exericese-2",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Exericese 2",
    "text": "Exericese 2\n\nOverlay multiple histograms in the same plot by cut using geom_freqpoly() \n\n \n\nggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +\n  geom_freqpoly(binwidth = 0.1)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#typical-values",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#typical-values",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Typical Values",
    "text": "Typical Values\n\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data.\nTo turn this information into useful questions, look for anything unexpected:\n\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan you see any unusual patterns? What might explain them?"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example",
    "text": "Example\n\nLook at the histogram below, what questions can you ask?\n\n\nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-solution",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-solution",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example: Solution",
    "text": "Example: Solution\n\nWhy are there more diamonds at whole carats and common fractions of carats?\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\nWhy are there no diamonds bigger than 3 carats?"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#unusual-values-outliers",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#unusual-values-outliers",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Unusual Values (Outliers)",
    "text": "Unusual Values (Outliers)\n\n\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern.\nSometimes outliers are data entry errors; other times outliers suggest important new science.\nWhen you have a lot of data, outliers are sometimes difficult to see in a histogram.\n\n\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = y), binwidth = 0.5)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#visualising-outliers",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#visualising-outliers",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Visualising Outliers",
    "text": "Visualising Outliers\n\nTo make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian() and ylim() or xlim() to zoom into the y-axis or x-axis.\n\n\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#display-all-the-unusual-values",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#display-all-the-unusual-values",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Display all the unusual values",
    "text": "Display all the unusual values\n\nWe pluck them out with dplyr.\nWhat questions you may have?\n\n\nunusual &lt;- diamonds %&gt;% \n  filter(y &lt; 3 | y &gt; 20) %&gt;% \n  select(price, x, y, z) %&gt;%\n  arrange(y)\nunusual"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#deal-with-outliers-example",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#deal-with-outliers-example",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Deal with Outliers: Example",
    "text": "Deal with Outliers: Example\n\nIn the ‘Diamond’ example, the y variable measures one of the three dimensions of these diamonds, in mm.\nWe know that diamonds can’t have a width of 0mm, so these values must be incorrect.\nWe might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars!"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#deal-with-outliers",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#deal-with-outliers",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Deal with Outliers",
    "text": "Deal with Outliers\n\nRepeat your analysis with and without the outliers.\nIf they have minimal effect on the results and you can’t figure out why they’re there, it’s reasonable to replace them with missing values and move on.\nIf they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up."
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#missing-values",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#missing-values",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Missing Values",
    "text": "Missing Values\n\n\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n\nDrop the entire row with the strange values (not recommend, why?)\n\n\n\ndiamonds2 &lt;- diamonds %&gt;% \n  filter(between(y, 3, 20))\n#diamonds2 \n\n\nReplace the unusual values with missing values (NA) using mutate() with ifelse() or case_when()\n\n\ndiamonds2 &lt;- diamonds %&gt;% \n  mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y))\n#diamonds2\n\n\nggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed."
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#covariation",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#covariation",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Covariation",
    "text": "Covariation\n\nIf variation describes the behavior within a variable, covariation describes the behavior between variables.\nCovariation is the tendency for the values of two or more variables to vary together in a related way.\nThe best way to spot covariation is to visualise the relationship between two or more variables.\nWe consider three different combinations\n\nA categorical and continuous variable\nTwo categorical variables\nTwo continuous variables"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#a-categorical-and-continuous-variable",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#a-categorical-and-continuous-variable",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "A Categorical and Continuous Variable",
    "text": "A Categorical and Continuous Variable\n\n\nExplore the distribution of a continuous variable broken down by a categorical variable\nusing geom_freqpoly(), for example see \nIt’s hard to see the difference in distribution because the overall counts differ so much, see .\nInstead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one.\n\n\nggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + \n  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#boxplot",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#boxplot",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Boxplot",
    "text": "Boxplot\n\nAnother alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot.\n\n\nBoxplot, source: R for Data Science"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-1-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-1-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example 1",
    "text": "Example 1\n\nTake a look at the distribution of diamond price by cut. What can you find?\n\n\nggplot(data = diamonds, mapping = aes(x = cut, y = price)) +\n  geom_boxplot()\n\n\n\nIt supports the counterintuitive finding that better quality diamonds are cheaper on average! Why?"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-2-1",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-2-1",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example 2",
    "text": "Example 2\n\nLook at the mpg dataset. We are interested to know how highway mileage (hwy) varies across classes (class)\nTo make the trend easier to see, we can reorder (reorder) class based on the median value (FUN=median) of hwy.\nIf you have long variable names, geom_boxplot() will work better if you flip it 90°. You can do that with coord_flip()\n\n\nggplot(data = mpg) +\n  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) #+\n\n#  coord_flip()"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#two-categorical-variables",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#two-categorical-variables",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Two Categorical Variables",
    "text": "Two Categorical Variables\n\nTo visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. One way to do that is to rely on the built-in geom_count()\n\n\nggplot(data = diamonds) +\n  geom_count(mapping = aes(x = cut, y = color))"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-3",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-3",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example",
    "text": "Example\n\nAnother approach is to compute the count with dplyr.\n\n\ndiamonds %&gt;% \n  count(color, cut)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-4",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-4",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example",
    "text": "Example\n\nThen visualise with geom_tile() and the fill aesthetic.\n\n\ndiamonds %&gt;% \n  count(color, cut) %&gt;%  \n  ggplot(mapping = aes(x = color, y = cut)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#two-continuous-variables",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#two-continuous-variables",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Two Continuous Variables",
    "text": "Two Continuous Variables\n\nOne great way to visualise the covariation between two continuous variables is to draw a scatter plot with geom_point(). You can see covariation as a pattern in the points.\nExample: visualise the relationship between the carat size and price of a diamond.\n\n\nggplot(data = diamonds) +\n  geom_point(mapping = aes(x = carat, y = price))"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#other-ways-to-visualisize-the-relationship",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#other-ways-to-visualisize-the-relationship",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Other ways to visualisize the relationship",
    "text": "Other ways to visualisize the relationship\n\nuse the alpha aesthetic to add transparency\nuse geom_bin2d() and geom_hex() to bin in two dimensions\nbin one continuous variable so it acts like a categorical variable"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-add-transparency",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#example-add-transparency",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "Example: add transparency",
    "text": "Example: add transparency\n\nggplot(data = diamonds) + \n  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)"
  },
  {
    "objectID": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#from-data-patterns-to-models",
    "href": "Lectures/Ch2-3-DataCleaning/Exploratory-Data-Analysis_Import-Quality-Check-and-Cleansing.slides.html#from-data-patterns-to-models",
    "title": "2.3 Importing, Quality Check and Cleansing",
    "section": "From Data Patterns to Models",
    "text": "From Data Patterns to Models\n\nPatterns in your data provide clues about relationships\nModels are a tool for extracting patterns out of data"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "",
    "text": "R for Data Science Online Book, Chapters 5, 13\n\nApplied predictive Modelling, 3.3 (only transformations to resolve outliers), 3.4",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.2 Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#introduction",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#introduction",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Introduction",
    "text": "Introduction\n\nMotivation\n\nIt is rare that you get the data in exactly the right form you need.\nYou’ll need to create some new variables or summaries, or\nyou just want to rename the variables or reorder the observations in order to make the data a little easier to work with.\n\n\n\nUsing R to manipulate data\n\nR package: dplyr (a core member of tidyverse) for data manipulation and transformation1\nData: nycflights13 package, flights departing New York City in 2013\nWe use ggplot2 to help us understand the data.\n\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nlibrary(tidyverse)\n\n\n\nData\n \n\nflights #tibble, tweaked data frame to work better in tidyverse\n\n\n  \n\n\nview(flights) # will open the dataset in the RStudio viewer\n\n\n\n\nTypes of variables\n\nint stands for integers.\ndbl stands for doubles, or real numbers.\nchr stands for character vectors, or strings.\ndttm stands for date-times (a date + a time).\nlgl stands for logical, vectors that contain only TRUE or FALSE.\nfctr stands for factors, which R uses to represent categorical variables with fixed possible values.\ndate stands for dates.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.2 Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#data-manipulation-functions",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#data-manipulation-functions",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Data Manipulation Functions",
    "text": "Data Manipulation Functions\n\nFunctions for data manipulation\nFunctions in dplyr package\n\n%&gt;% Pipe operator\nglimpse() A glimpse into the data and its structure\nfilter() Pick observations by their values\n\narrange() Reorder the rows\n\nselect() Pick variables by their names\n\nsummarise() Collapse many values down to a single\n\ngroup_by() Changes the scope of each function above from operating on the entire dataset to operating on it group-by-group\n\n\n\nFilter: Introduction\n \n\n#The first argument is the name of the data frame. \n#The subsequent arguments are the expressions that filter the data frame.\nfilter(flights, month == 1, day == 1)\n\n\n  \n\n\n# use the assignment operator, &lt;- to save the result\n#jan1 &lt;- filter(flights, month == 1, day == 1)\n# Save and print the result at the same time\n#(dec25 &lt;- filter(flights, month == 12, day == 25))\n\n\n\n\nFilter: Comparisons\nTo use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite:\n\n&gt; bigger than\n\n&gt;= bigger than or equal to\n&lt; less than\n\n&lt;= less than or equal to\n!= not equal\n== equal2\n\n\n\nFilter: Logical Operators\n\nMultiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output.\n\nOther types of combinations using Boolean operators:\n\n& is “and”\n| is “or”\n! is “not”\nx %in% y select every row where x is one of the values in y\n\nAccording to De Morgan’s law,\n\n!(x & y) is the same as !x | !y\n!(x | y) is the same as !x & !y\n\n\n\nExercise 1\n\nFind all flights that departed in November or December. \n\n\nfilter(flights, month %in% c(11, 12))\n\n\n  \n\n\n#Alternatively, use the following\n#filter(flights, month == 11 | month == 12)\n\n\n\nExercise 2\n\nFind flights that weren’t delayed (on arrival or departure) by more than two hours. \n\n\nfilter(flights, !(arr_delay &gt; 120 | dep_delay &gt; 120))\n\n\n  \n\n\n#Alternatively, use the following\n#filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120)\n\n\n\nFilter: Missing Values\n\nNA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown.\nfilter() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. If you want to preserve missing values, ask for them explicitly:\n\n\ndf &lt;- tibble(x = c(1, NA, 3))\nfilter(df, x &gt; 1)\n\n\n  \n\n\nfilter(df, is.na(x) | x &gt; 1)\n\n\n  \n\n\n\n\n\nArrange\n\narrange(): Order by column names (or more complicated expressions)\nUse desc() to re-order by a column in descending order\nMissing values are always sorted at the end\n\n\narrange(flights, year)\n\n\n  \n\n\n#arrange(flights, desc(dep_delay))\n\n\n\nSelect\n\nselect() allows you to select a useful subset based on the names of the variables.\n\n\nselect(flights, year, month, day)\n\n\n  \n\n\n#select(flights, year:day)\n#select(flights, -(year:day))\n\n\n\nSelect: Useful functions\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nmatches(): selects variables that match a regular expression.\neverything()\n\nOther related functions:\n\nrename(): rename variables\nmutate(): create new variables with functions of existing variables\n\n\n\nSummarise\n\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE))\n\n\n  \n\n\n\n\n\nSummarise: with group-by\n\nThis changes the unit of analysis from the complete dataset to individual groups.\n\n\nby_day &lt;- group_by(flights, year, month, day)\nsummarise(by_day, delay = mean(dep_delay, na.rm = TRUE))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\n\n\nSummarise: Exercise with multiple operations\n\nExplore the relationship between the distance and average delay for each location\nThere are three steps to prepare this data:\n\nGroup flights by destination\nSummarise to compute distance, average delay, and number of flights\nFilter to remove noisy points (counts below or equal to 20) and Honolulu (“HNL”) airport, which is almost twice as far away as the next closest airport\n\n\nPlease have a try!\n\n\nExercise: code\n\nby_dest &lt;- group_by(flights, dest)\ndelay &lt;- summarise(by_dest,\n  count = n(),\n  dist = mean(distance, na.rm = TRUE),\n  delay = mean(arr_delay, na.rm = TRUE)\n)\ndelay &lt;- filter(delay, count &gt; 20, dest != \"HNL\")\n\n# It looks like delays increase with distance up to ~750 miles \n# and then decrease. Maybe as flights get longer there's more \n# ability to make up delays in the air?\nplot=ggplot(data = delay, mapping = aes(x = dist, y = delay)) +\n  geom_point(aes(size = count), alpha = 1/3) +\n  geom_smooth(se = FALSE)\n#&gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\nExercise: plot\n\nprint(plot)\n\n\n\n\n\n\n\n\n\n\nExercise: Another way to do it using pipe %&gt;%\n\ndelays &lt;- flights %&gt;% \n  group_by(dest) %&gt;% \n  summarise(\n    count = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %&gt;% \n  filter(count &gt; 20, dest != \"HNL\")\n\n\n\nMissing values\n\nna.rm=TRUE removes the missing values\n\n\nflights %&gt;% \n  group_by(year, month, day) %&gt;% \n  summarise(mean = mean(dep_delay, na.rm = TRUE))\n\n\n  \n\n\n\n\n\nUseful Summary functions\n\nMeasures of location (central tendency): mean(x), median(x)\nMeasures of spread (variability): sd(x), IQR(x)3\nMeasures of rank: min(x), quantile(x, 0.25), max(x)\nMeasures of position: first(x), nth(x, 2), last(x)\nCounts: n(), sum(!is.na(x)), n_distinct(x)\n\ncount(tailnum, wt = distance), “count” (sum) the total number of miles a plane flew\n\nCounts and proportions of logical values: sum(x &gt; 10), mean(y == 0)\n\n\n\nGrouping by multiple variables\n\ndaily &lt;- group_by(flights, year, month, day)\n#n() returns the size of the current group\n(per_day   &lt;- summarise(daily, flights = n()))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\n\n\nUngrouping\n\ndaily %&gt;% \n  ungroup() %&gt;%             # no longer grouped by date\n  summarise(flights = n())  # all flights",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.2 Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#relational-data",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#relational-data",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Relational Data",
    "text": "Relational Data\n\nRelational Data\n\nRelational data: multiple tables of data that are related.\n\nThree families of verbs designed to work with relational data:\n\nMutating joins: add new variables to one data frame from matching observations in another.\nFiltering joins: filter observations from one data frame based on whether or not they match an observation in the other table.\n\nSet operations: treat observations as if they were set elements.\nOther similar database system: SQL\n\n\n\nData set\n\nnycflights13 contains five tibbles that are related to each other:\n\nflights: gives information about each flight\nairlines: lets you look up the full carrier name\nairports: gives information about each airport, identified by the faa airport code\nplanes: gives information about each plane, identified by its tailnum\nweather: gives the weather at each NYC airport for each hour\n\n\n\n\nTable Relations\n\nEach relation always concerns a pair of tables\nUnderstand the chain of relations between the tables that you are interested in.\n\n\n\n\nRelations of Tibbles\n\n\n\n\nKeys\n\nPrimary key: uniquely identifies an observation in its own table.\n\nFor example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.\n\nForeign key: uniquely identifies an observation in another table.\n\nFor example, flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.\n\nA variable can be both a primary key and a foreign key. For example, origin is part of the weather primary key, and is also a foreign key for the airport table.\n\n\n\nIdentify the primary keys\n\ncount() the primary keys and look for entries where n is greater than one.\n\n\nplanes %&gt;% \n  count(tailnum) %&gt;% \n  filter(n &gt; 1)\n\n\n  \n\n\n\n\n\nAdd a primary key\n\nWhat’s the primary key in the flights table?\n\nNone\n\nSurrogate key:add one primary key with mutate() and row_number()\nA primary key and the corresponding foreign key in another table form a relation.\nRelations are typically one-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation.\n\n\n\nExercise: Add a surrogate key to flights\n\nflights %&gt;% \n  arrange(year, month, day, sched_dep_time, carrier, flight) %&gt;%\n  mutate(flight_id = row_number()) %&gt;%\n  #TThis makes it possible to see every column in a data frame.\n  glimpse()\n\nRows: 336,776\nColumns: 20\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 559, 558, 559, 558, 558, 557, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 558, 559, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -4, 0, -2, -1, -2, -2, -3, NA, 1, 0, -5, -…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 740, 702, 753, 941, 849, 853, 838,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 728, 706, 745, 910, 851, 856, 846,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, 12, -4, 8, 31, -2, -3, -8, NA, -6, -7,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"UA\", \"B6\", \"AA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 1696, 1806, 301, 707, 49, 71, 79…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N39463\", \"N708…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"EWR\", \"JFK\", \"LGA\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ORD\", \"BOS\", \"ORD\", \"DFW\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 150, 44, 138, 257, 149, 158, 140, N…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 719, 187, 733, 1389, 1028, 1005…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 58, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n$ flight_id      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n\n\n\n\nMutating Joins\n\nMutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other.\nAdd columns from y to x:\n\ninner_join(): keeps observations that appear in both tables.\nleft_join(): keeps all observations in x.\nright_join(): keeps all observations in y.\nfull_join(): keeps all observations in x and y.\n\n\n\nflights %&gt;% \n  select(year:day, hour, tailnum, carrier) %&gt;% \n  left_join(airlines, by = \"carrier\") #by = \"key\"\n\n\n  \n\n\n\n\n\nMutating Joins: The Key Columns\n\nWhen you join duplicated keys, you get all possible combinations.\nDefining the key columns\n\nby=NULL: uses all variables that appear in both tables, the so called natural join\nby = \"x\": uses only some of the common variables.\nby = c(\"a\" = \"b\"): match variable a in table x to variable b in table y\n\n\n\n\nMutating Joins: base::merge()\n\n\n\ndplyr\nmerge\n\n\n\n\ninner_join(x, y)\nmerge(x, y)\n\n\nleft_join(x, y)\nmerge(x, y, all.x = TRUE)\n\n\nright_join(x, y)\nmerge(x, y, all.y = TRUE)\n\n\nfull_join(x, y)\nmerge(x, y, all.x = TRUE, all.y = TRUE)\n\n\n\n\ndplyr’s joins are considerably faster and don’t mess with the order of the rows.\n\n\n\nFiltering Joins\n\nFiltering joins match observations in the same way as mutating joins, but affect the observations, not the variables.\nThere are two types:\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\nanti_join(x, y) drops all observations in x that have a match in y.\n\n\n\n\nExercise 1\n\nQuestions:\n\nfind the top ten most popular destinations\nmatch it back to flights \n\n\n\ntop_dest &lt;- flights %&gt;%\n  count(dest, sort = TRUE) %&gt;%\n  head(10) \n\nflights %&gt;%\n  semi_join(top_dest)\n\nJoining with `by = join_by(dest)`\n\n\n\n  \n\n\n\n\n\nExercise 2\n\nQuestion: when connecting flights and planes, what are the flights that don’t have a match in planes? \n\n\nflights %&gt;%\n  anti_join(planes, by = \"tailnum\") %&gt;%\n  count(tailnum, sort = TRUE)\n\n\n  \n\n\n\n\n\nJoin problems\n\nStart by identifying the variables that form the primary key in each table.\nCheck that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation!\nCheck that your foreign keys match primary keys in another table. The best way to do this is with an anti_join().\n\n\n\nSet Operations\n\nintersect(x, y): return only observations in both x and y.\nunion(x, y): return unique observations in x and y.\nsetdiff(x, y): return observations in x, but not in y.\n\n\n\nExamples\n\ndf1 &lt;- tribble(\n  ~x, ~y,\n   1,  1,\n   2,  1\n)\ndf2 &lt;- tribble(\n  ~x, ~y,\n   1,  1,\n   1,  2\n)\nintersect(df1, df2)\n\n\n  \n\n\nunion(df1, df2)\n\n\n  \n\n\nsetdiff(df1, df2) #setdiff(df2, df1)",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.2 Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#footnotes",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.html#footnotes",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ndplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag()↩︎\nBe cautions when using ==: floating point numbers. Consider using near()↩︎\nThe interquartile range (IQR) is a measure of variability, based on dividing a data set into quartiles. Q1 is the “middle” value in the first half of the rank-ordered data set. Q2 is the median value in the set. Q3 is the “middle” value in the second half of the rank-ordered data set. The interquartile range is equal to Q3 minus Q1.↩︎",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.2 Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#reading-list",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#reading-list",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Reading List",
    "text": "Reading List\n\nR for Data Science Online Book, Chapters 5, 13\n\nApplied predictive Modelling, 3.3 (only transformations to resolve outliers), 3.4"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#motivation",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#motivation",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Motivation",
    "text": "Motivation\n\nIt is rare that you get the data in exactly the right form you need.\nYou’ll need to create some new variables or summaries, or\nyou just want to rename the variables or reorder the observations in order to make the data a little easier to work with."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#using-r-to-manipulate-data",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#using-r-to-manipulate-data",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Using R to manipulate data",
    "text": "Using R to manipulate data\n\nR package: dplyr (a core member of tidyverse) for data manipulation and transformation1\nData: nycflights13 package, flights departing New York City in 2013\nWe use ggplot2 to help us understand the data.\n\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nlibrary(tidyverse)\n\ndplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag()"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#data",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#data",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Data",
    "text": "Data\n \n\nflights #tibble, tweaked data frame to work better in tidyverse\n\n\n  \n\n\nview(flights) # will open the dataset in the RStudio viewer"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#types-of-variables",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#types-of-variables",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Types of variables",
    "text": "Types of variables\n\nint stands for integers.\ndbl stands for doubles, or real numbers.\nchr stands for character vectors, or strings.\ndttm stands for date-times (a date + a time).\nlgl stands for logical, vectors that contain only TRUE or FALSE.\nfctr stands for factors, which R uses to represent categorical variables with fixed possible values.\ndate stands for dates."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#functions-for-data-manipulation",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#functions-for-data-manipulation",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Functions for data manipulation",
    "text": "Functions for data manipulation\nFunctions in dplyr package\n\n%&gt;% Pipe operator\nglimpse() A glimpse into the data and its structure\nfilter() Pick observations by their values\n\narrange() Reorder the rows\n\nselect() Pick variables by their names\n\nsummarise() Collapse many values down to a single\n\ngroup_by() Changes the scope of each function above from operating on the entire dataset to operating on it group-by-group"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-introduction",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-introduction",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Filter: Introduction",
    "text": "Filter: Introduction\n \n\n#The first argument is the name of the data frame. \n#The subsequent arguments are the expressions that filter the data frame.\nfilter(flights, month == 1, day == 1)\n\n\n  \n\n\n# use the assignment operator, &lt;- to save the result\n#jan1 &lt;- filter(flights, month == 1, day == 1)\n# Save and print the result at the same time\n#(dec25 &lt;- filter(flights, month == 12, day == 25))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-comparisons",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-comparisons",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Filter: Comparisons",
    "text": "Filter: Comparisons\nTo use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite:\n\n&gt; bigger than\n\n&gt;= bigger than or equal to\n&lt; less than\n\n&lt;= less than or equal to\n!= not equal\n== equal1\n\nBe cautions when using ==: floating point numbers. Consider using near()"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-logical-operators",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-logical-operators",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Filter: Logical Operators",
    "text": "Filter: Logical Operators\n\nMultiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output.\n\nOther types of combinations using Boolean operators:\n\n& is “and”\n| is “or”\n! is “not”\nx %in% y select every row where x is one of the values in y\n\nAccording to De Morgan’s law,\n\n!(x & y) is the same as !x | !y\n!(x | y) is the same as !x & !y"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-1",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-1",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nFind all flights that departed in November or December. \n\n\nfilter(flights, month %in% c(11, 12))\n\n\n  \n\n\n#Alternatively, use the following\n#filter(flights, month == 11 | month == 12)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-2",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-2",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nFind flights that weren’t delayed (on arrival or departure) by more than two hours. \n\n\nfilter(flights, !(arr_delay &gt; 120 | dep_delay &gt; 120))\n\n\n  \n\n\n#Alternatively, use the following\n#filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-missing-values",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filter-missing-values",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Filter: Missing Values",
    "text": "Filter: Missing Values\n\nNA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown.\nfilter() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. If you want to preserve missing values, ask for them explicitly:\n\n\ndf &lt;- tibble(x = c(1, NA, 3))\nfilter(df, x &gt; 1)\n\n\n  \n\n\nfilter(df, is.na(x) | x &gt; 1)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#arrange",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#arrange",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Arrange",
    "text": "Arrange\n\narrange(): Order by column names (or more complicated expressions)\nUse desc() to re-order by a column in descending order\nMissing values are always sorted at the end\n\n\narrange(flights, year)\n\n\n  \n\n\n#arrange(flights, desc(dep_delay))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#select",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#select",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Select",
    "text": "Select\n\nselect() allows you to select a useful subset based on the names of the variables.\n\n\nselect(flights, year, month, day)\n\n\n  \n\n\n#select(flights, year:day)\n#select(flights, -(year:day))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#select-useful-functions",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#select-useful-functions",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Select: Useful functions",
    "text": "Select: Useful functions\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nmatches(): selects variables that match a regular expression.\neverything()\n\nOther related functions:\n\nrename(): rename variables\nmutate(): create new variables with functions of existing variables"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#summarise",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#summarise",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Summarise",
    "text": "Summarise\n\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#summarise-with-group-by",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#summarise-with-group-by",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Summarise: with group-by",
    "text": "Summarise: with group-by\n\nThis changes the unit of analysis from the complete dataset to individual groups.\n\n\nby_day &lt;- group_by(flights, year, month, day)\nsummarise(by_day, delay = mean(dep_delay, na.rm = TRUE))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#summarise-exercise-with-multiple-operations",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#summarise-exercise-with-multiple-operations",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Summarise: Exercise with multiple operations",
    "text": "Summarise: Exercise with multiple operations\n\nExplore the relationship between the distance and average delay for each location\nThere are three steps to prepare this data:\n\nGroup flights by destination\nSummarise to compute distance, average delay, and number of flights\nFilter to remove noisy points (counts below or equal to 20) and Honolulu (“HNL”) airport, which is almost twice as far away as the next closest airport\n\n\nPlease have a try!"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-code",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-code",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise: code",
    "text": "Exercise: code\n\nby_dest &lt;- group_by(flights, dest)\ndelay &lt;- summarise(by_dest,\n  count = n(),\n  dist = mean(distance, na.rm = TRUE),\n  delay = mean(arr_delay, na.rm = TRUE)\n)\ndelay &lt;- filter(delay, count &gt; 20, dest != \"HNL\")\n\n# It looks like delays increase with distance up to ~750 miles \n# and then decrease. Maybe as flights get longer there's more \n# ability to make up delays in the air?\nplot=ggplot(data = delay, mapping = aes(x = dist, y = delay)) +\n  geom_point(aes(size = count), alpha = 1/3) +\n  geom_smooth(se = FALSE)\n#&gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-plot",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-plot",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise: plot",
    "text": "Exercise: plot\n\nprint(plot)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-another-way-to-do-it-using-pipe",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-another-way-to-do-it-using-pipe",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise: Another way to do it using pipe %>%",
    "text": "Exercise: Another way to do it using pipe %&gt;%\n\ndelays &lt;- flights %&gt;% \n  group_by(dest) %&gt;% \n  summarise(\n    count = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %&gt;% \n  filter(count &gt; 20, dest != \"HNL\")"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#missing-values",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#missing-values",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Missing values",
    "text": "Missing values\n\nna.rm=TRUE removes the missing values\n\n\nflights %&gt;% \n  group_by(year, month, day) %&gt;% \n  summarise(mean = mean(dep_delay, na.rm = TRUE))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#useful-summary-functions",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#useful-summary-functions",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Useful Summary functions",
    "text": "Useful Summary functions\n\nMeasures of location (central tendency): mean(x), median(x)\nMeasures of spread (variability): sd(x), IQR(x)1\nMeasures of rank: min(x), quantile(x, 0.25), max(x)\nMeasures of position: first(x), nth(x, 2), last(x)\nCounts: n(), sum(!is.na(x)), n_distinct(x)\n\ncount(tailnum, wt = distance), “count” (sum) the total number of miles a plane flew\n\nCounts and proportions of logical values: sum(x &gt; 10), mean(y == 0)\n\nThe interquartile range (IQR) is a measure of variability, based on dividing a data set into quartiles. Q1 is the “middle” value in the first half of the rank-ordered data set. Q2 is the median value in the set. Q3 is the “middle” value in the second half of the rank-ordered data set. The interquartile range is equal to Q3 minus Q1."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#grouping-by-multiple-variables",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#grouping-by-multiple-variables",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Grouping by multiple variables",
    "text": "Grouping by multiple variables\n\ndaily &lt;- group_by(flights, year, month, day)\n#n() returns the size of the current group\n(per_day   &lt;- summarise(daily, flights = n()))"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#ungrouping",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#ungrouping",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Ungrouping",
    "text": "Ungrouping\n\ndaily %&gt;% \n  ungroup() %&gt;%             # no longer grouped by date\n  summarise(flights = n())  # all flights"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#relational-data-1",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#relational-data-1",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Relational Data",
    "text": "Relational Data\n\nRelational data: multiple tables of data that are related.\n\nThree families of verbs designed to work with relational data:\n\nMutating joins: add new variables to one data frame from matching observations in another.\nFiltering joins: filter observations from one data frame based on whether or not they match an observation in the other table.\n\nSet operations: treat observations as if they were set elements.\nOther similar database system: SQL"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#data-set",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#data-set",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Data set",
    "text": "Data set\n\nnycflights13 contains five tibbles that are related to each other:\n\nflights: gives information about each flight\nairlines: lets you look up the full carrier name\nairports: gives information about each airport, identified by the faa airport code\nplanes: gives information about each plane, identified by its tailnum\nweather: gives the weather at each NYC airport for each hour"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#table-relations",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#table-relations",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Table Relations",
    "text": "Table Relations\n\nEach relation always concerns a pair of tables\nUnderstand the chain of relations between the tables that you are interested in.\n\n\nRelations of Tibbles"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#keys",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#keys",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Keys",
    "text": "Keys\n\nPrimary key: uniquely identifies an observation in its own table.\n\nFor example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.\n\nForeign key: uniquely identifies an observation in another table.\n\nFor example, flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.\n\nA variable can be both a primary key and a foreign key. For example, origin is part of the weather primary key, and is also a foreign key for the airport table."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#identify-the-primary-keys",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#identify-the-primary-keys",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Identify the primary keys",
    "text": "Identify the primary keys\n\ncount() the primary keys and look for entries where n is greater than one.\n\n\nplanes %&gt;% \n  count(tailnum) %&gt;% \n  filter(n &gt; 1)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#add-a-primary-key",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#add-a-primary-key",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Add a primary key",
    "text": "Add a primary key\n\nWhat’s the primary key in the flights table?\n\nNone\n\nSurrogate key:add one primary key with mutate() and row_number()\nA primary key and the corresponding foreign key in another table form a relation.\nRelations are typically one-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-add-a-surrogate-key-to-flights",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-add-a-surrogate-key-to-flights",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise: Add a surrogate key to flights",
    "text": "Exercise: Add a surrogate key to flights\n\nflights %&gt;% \n  arrange(year, month, day, sched_dep_time, carrier, flight) %&gt;%\n  mutate(flight_id = row_number()) %&gt;%\n  #TThis makes it possible to see every column in a data frame.\n  glimpse()\n\nRows: 336,776\nColumns: 20\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 559, 558, 559, 558, 558, 557, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 558, 559, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -4, 0, -2, -1, -2, -2, -3, NA, 1, 0, -5, -…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 740, 702, 753, 941, 849, 853, 838,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 728, 706, 745, 910, 851, 856, 846,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, 12, -4, 8, 31, -2, -3, -8, NA, -6, -7,…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"UA\", \"B6\", \"AA\", \"AA\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 1696, 1806, 301, 707, 49, 71, 79…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N39463\", \"N708…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"EWR\", \"JFK\", \"LGA\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ORD\", \"BOS\", \"ORD\", \"DFW\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 150, 44, 138, 257, 149, 158, 140, N…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 719, 187, 733, 1389, 1028, 1005…\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 58, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n$ flight_id      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#mutating-joins",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#mutating-joins",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Mutating Joins",
    "text": "Mutating Joins\n\nMutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other.\nAdd columns from y to x:\n\ninner_join(): keeps observations that appear in both tables.\nleft_join(): keeps all observations in x.\nright_join(): keeps all observations in y.\nfull_join(): keeps all observations in x and y.\n\n\n\nflights %&gt;% \n  select(year:day, hour, tailnum, carrier) %&gt;% \n  left_join(airlines, by = \"carrier\") #by = \"key\""
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#mutating-joins-the-key-columns",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#mutating-joins-the-key-columns",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Mutating Joins: The Key Columns",
    "text": "Mutating Joins: The Key Columns\n\nWhen you join duplicated keys, you get all possible combinations.\nDefining the key columns\n\nby=NULL: uses all variables that appear in both tables, the so called natural join\nby = \"x\": uses only some of the common variables.\nby = c(\"a\" = \"b\"): match variable a in table x to variable b in table y"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#mutating-joins-basemerge",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#mutating-joins-basemerge",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Mutating Joins: base::merge()",
    "text": "Mutating Joins: base::merge()\n\n\n\ndplyr\nmerge\n\n\n\n\ninner_join(x, y)\nmerge(x, y)\n\n\nleft_join(x, y)\nmerge(x, y, all.x = TRUE)\n\n\nright_join(x, y)\nmerge(x, y, all.y = TRUE)\n\n\nfull_join(x, y)\nmerge(x, y, all.x = TRUE, all.y = TRUE)\n\n\n\n\ndplyr’s joins are considerably faster and don’t mess with the order of the rows."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filtering-joins",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#filtering-joins",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nFiltering joins match observations in the same way as mutating joins, but affect the observations, not the variables.\nThere are two types:\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\nanti_join(x, y) drops all observations in x that have a match in y."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-1-1",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-1-1",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nQuestions:\n\nfind the top ten most popular destinations\nmatch it back to flights \n\n\n\ntop_dest &lt;- flights %&gt;%\n  count(dest, sort = TRUE) %&gt;%\n  head(10) \n\nflights %&gt;%\n  semi_join(top_dest)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-2-1",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#exercise-2-1",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nQuestion: when connecting flights and planes, what are the flights that don’t have a match in planes? \n\n\nflights %&gt;%\n  anti_join(planes, by = \"tailnum\") %&gt;%\n  count(tailnum, sort = TRUE)"
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#join-problems",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#join-problems",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Join problems",
    "text": "Join problems\n\nStart by identifying the variables that form the primary key in each table.\nCheck that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation!\nCheck that your foreign keys match primary keys in another table. The best way to do this is with an anti_join()."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#set-operations",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#set-operations",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Set Operations",
    "text": "Set Operations\n\nintersect(x, y): return only observations in both x and y.\nunion(x, y): return unique observations in x and y.\nsetdiff(x, y): return observations in x, but not in y."
  },
  {
    "objectID": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#examples",
    "href": "Lectures/Ch2-2-DataManipulation/Exploratory Data Analysis_Manipulation and Transformation.slides.html#examples",
    "title": "2.2 Data Manipulation and Transformation",
    "section": "Examples",
    "text": "Examples\n\ndf1 &lt;- tribble(\n  ~x, ~y,\n   1,  1,\n   2,  1\n)\ndf2 &lt;- tribble(\n  ~x, ~y,\n   1,  1,\n   1,  2\n)\nintersect(df1, df2)\n\n\n  \n\n\nunion(df1, df2)\n\n\n  \n\n\nsetdiff(df1, df2) #setdiff(df2, df1)"
  },
  {
    "objectID": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html",
    "href": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html",
    "title": "Lab: R Markdown Introduction",
    "section": "",
    "text": "Learn how to use R Markdown to write an academic report.",
    "crumbs": [
      "Chapter 1 - Introduction",
      "Lab: R Markdown Introduction"
    ]
  },
  {
    "objectID": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#learning-objectives",
    "href": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#learning-objectives",
    "title": "Lab: R Markdown Introduction",
    "section": "",
    "text": "Learn how to use R Markdown to write an academic report.",
    "crumbs": [
      "Chapter 1 - Introduction",
      "Lab: R Markdown Introduction"
    ]
  },
  {
    "objectID": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#r-markdown-introduction",
    "href": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#r-markdown-introduction",
    "title": "Lab: R Markdown Introduction",
    "section": "R Markdown Introduction",
    "text": "R Markdown Introduction\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. To learn more details on using R Markdown, please refer to R Markdown Cookbook and R Markdown: The Definitive Guide. After attending this Lab session, students are expected to learn basic skills in writing documents using R Markdown.\nThere are many advantages to use R Markdown in your work. An important benefit is that it allows your R code and/or its outputs to be directly embedded into formatted documents for communication and provides quick and reproducible reporting from R. However, when using Latex or Word to write a paper or assignment, the codes and data are run in a separate place and you need to manually insert all outputs, such as figures and tables, into the document. And when there is a change of results due to the change of data or code, you need to update all related outputs manually in your report, which can be time consuming and easy to make mistakes. However, R Markdown allows all outputs to be automatically updated when there is a change. Another advantage is that it is incredibly easy to use. R Markdown uses simple syntax and can be learned quickly.\n\nInstallation\nBefore you install R Markdown, you need have R and RStudio downloaded. First, to install R, go to this link R. Remember to choose a correct version for your system. Then, to install RStudio, go to this link, RStudio. Also remember to install a correct version for your system.\nNow open RStudio to install R Markdown. R Markdown files are designed to be used with the rmarkdown package. rmarkdown comes installed with the RStudio IDE, but you can acquire your own copy of rmarkdown from CRAN with the command.\n\n# install.packages(\"rmarkdown\")\n\nNote: RStudio does not build PDF and Word documents from scratch. You will need to have a distribution of Latex installed on your computer to make PDFs and Microsoft Word (or a similar program) installed to make Word files.\nAccording to the R Markdown Cook Book,\n“If you would like to create PDF documents from R Markdown, you will need to have a LaTeX distribution installed. Although there are several traditional options including MiKTeX, MacTeX, and TeX Live, we recommend that R Markdown users install TinyTeX.”\n\n#To install TinyTex, uncomment and run the following code\n#tinytex::install_tinytex()\n\n# to uninstall TinyTeX, uncomment and run the following code\n#tinytex::uninstall_tinytex()\n\nOpen your R Studio, Click “File” -&gt; “New File” -&gt; “R Markdown”. Then you will see this:\n\nknitr::include_graphics('rmarkdown window.jpg')\n\n\n\n\n\n\n\n\nHere you can choose an output format, and it’s not important because you can change the format anytime later.\n\n\nKnit & Convert\nR Markdown files are the source code for rich, reproducible documents. You can transform an R Markdown file in two ways.\n\nknit - You can knit the file. The rmarkdown package will call the knitr package. knitr will run each chunk of R code in the document and append the results of the code to the document next to the code chunk. This workflow saves time and facilitates reproducible reports. Consider how authors typically include graphs (or tables, or numbers) in a report. The author makes the graph, saves it as a file, and then copy and pastes it into the final report. This process relies on manual labor. If the data changes, the author must repeat the entire process to update the graph. In the R Markdown paradigm, each report contains the code it needs to make its own graphs, tables, numbers, etc. The author can automatically update the report by re-knitting.\nconvert - You can convert the file. The rmarkdown package will use the pandoc program to transform the file into a new format. For example, you can convert your ‘.Rmd’ file into an HTML, PDF, or Microsoft Word file. You can even turn the file into an HTML5 or PDF slideshow. rmarkdown will preserve the text, code results, and formatting contained in your original .Rmd file. Conversion lets you do your original work in markdown, which is very easy to use. You can include R code to knit, and you can share your document in a variety of formats.\n\nIn practice, authors almost always knit and convert their documents at the same time. In this article, I will use the term render to refer to the two step process of knitting and converting an R Markdown file.\n\n\n\n\n\n\nTask\n\n\n\nKnit the empty template called template.Rmd and review the output.\n\n\n\n\nThree Frameworks\nR Markdown reports rely on three frameworks.\n\nmarkdown for formatted text.\nknitr for embedded R code.\nYAML for render parameters.\n\n\nMarkdown for Formatted Text\nMarkdown is a set of conventions for formatting plain text. You can use markdown to indicate\n\nbold and italic text: Surround italicized text with asterisks, like this without realizing it. Surround bold text with two asterisks, like this easy to use.\nlists: Group lines into bullet points that begin with asterisks. Leave a blank line before the first bullet.\nheaders (e.g., section titles): Place one or more hashtags at the start of a line that will be a header (or sub-header). For example, # Say Hello to markdown. A single hashtag creates a first level header. Two hashtags, ##, creates a second level header, and so on.\nhyperlinks: Github, StackOverflow, Reddit\nand much more\n\nThe conventions of markdown are very unobtrusive, which make Markdown files easy to read. The file below uses several of the most useful markdown conventions.\nTo access a quick guide, click -&gt; “Help” -&gt; “Markdown Quick Reference”. RStudio will open the Markdown Quick Reference guide in the Help pane.\nA more comprehensive guide can be found, click -&gt; “Help” -&gt; “Cheatsheets” -&gt; “R Markdown Cheat Sheet”.\n\n\nknitr for embedded R code\nThe knitr package extends the basic markdown syntax to include chunks of executable R code.\nWhen you render the report, knitr will run the code and add the results to the output file. You can have the output display just the code, just the results, or both.\nTo embed a chunk of R code into your report, surround the code with two lines that each contain three backticks. After the first set of backticks, include {r}, which alerts knitr that you have included a chunk of R code. A shortcut to embed a chunk is to press “ALT + CTRL + I”. The result will look like this\n\ndim(iris) # Retrieve or set the dimension of an object.\n\n[1] 150   5\n\niris[3,1] # The object at row 3 co\n\n[1] 4.7\n\n\n\n\nYAML for render parameters\ntitle: “your awesome title” author: “your name” date: “today” or type any time you want.\nThe output: value determines what type of output to convert the file into when you call rmarkdown::render(). Note: you do not need to specify output: if you render your file with the RStudio IDE knit button.\noutput: recognizes the following values:\n\nhtml_document, which will create HTML output (default)\npdf_document, which will create PDF output\nword_document, which will create Word output\n\nYou can also use the output: value to render your document as a slideshow.\noutput: ioslides_presentation will create an ioslides (HTML5) slideshow output: beamer_presentation will create a beamer (PDF) slideshow\nHowever, you can see the output of this file is bookdown::html_document2( a bookdown output format).\nThe reason is that cross-referencing is not provided directly within the base rmarkdown package, but is provided as an extension in bookdown. We must therefore use an output format from bookdown (e.g., html_document2, pdf_document2, and `word_document2,etc.) in the YAML output field. We will talk about cross-referencing in the last section.\nNow let me illustrate this template of header in detail. You can create a cool template yourself. Here is good resource for learning (https://bookdown.org/yihui/rmarkdown/).\nYou can add a table of contents (TOC) using the toc option and specify the depth of headers that it applies to using the toc_depth option. For example:\ntoc: yes\ntoc_depth: ‘3’\nYou can add section numbering to headers using the number_sections option:\nnumber_sections: yes\nfig_caption=yes controls whether figures are rendered with captions.\ncode_folding: hide : hide option enables you to include R code but have it hidden by default. Users can then choose to show hidden R code chunks either individually or document wide. You can specify code_folding: show to still show all R code by default but then allow users to hide the code if they wish. How to do it? I will show you in the next section.\n\nRendering\nTo transform your markdown file into an HTML, PDF, or Word document, click the “Knit” icon that appears above your file in the scripts editor. A drop down menu will let you select the type of output that you want. Let’s try all of them. A shortcut for Windows is to press “SHIFT + CTRL +K”. (The short-cut to render in mac is SHIFT+COMMAND+K)\nWhen you click the button, rmarkdown will duplicate your text in the new file format. rmarkdown will use the formatting instructions that you provided with markdown syntax. Once the file is rendered, RStudio will show you a preview of the new output and save the output file in your working directory.\n\n\n\nTask: change the output to html and Knit it\nHint: go to the header (top).\n\n\n\nChunks\nNow, you are familiar with R Markdown and it’s time to practice. R Markdown is a variant of Markdown that has embedded R code chunks, to be used with knitr to make it easy to create reproducible web-based reports.\n\nCode chunks\nNow, let’s use “CTRL + ALT + I” to create an empty chunk. (A shortcut to embed a chunk is in mac is OPTION+COMMAND+I)\nIt’s usually best to give each code chunk a name, like simulate-data above. The name is optional; if included, each code chunk needs a distinct name. The advantage of giving each chunk a name is that it will be easier to understand where to look for errors, should they occur. Also, any figures that are created will be given names based on the name of the code chunk that produced them.\n\n\nChunk options\nThe initial line in a code chunk may include various options.\n\necho=FALSE indicates that the code will not be shown in the final document (though any results/output would still be displayed). Remember in the header we already set code_folding: hide. If you want to show codes for specific chunk you can set echo=TRUE for that chunk.\nYou use results='hide'to hide the results/output (but here the code would still be displayed).\nYou use include=FALSE to have the chunk evaluated, but neither the code nor its output displayed. If I’m writing a report for a collaborator, I’ll often use include=FALSE to suppress all of the code and largely just include figures.\nwarning=FALSE and message=FALSE suppress any R warnings or messages from being included in the final document.\n\n\nset.seed(123)\nx &lt;- rnorm(100) # 100 random variables from a standard normal distribution\ny &lt;- 2*x + rnorm(100)\ncor(x, y)\n\nFor figures, you’ll want to use options like fig.width and fig.height. You can see the output of this particular chunk by clicking the “green play button” called Run Current Chunk. Have a try here:\n\nplot(x,y)\n\n\n\n\nScatterplot\n\n\n\n\nNote that if include=FALSE, all of the code, results, and figures will be suppressed. If include=TRUE and results=\"hide\", the results will be hidden but figures will still be shown. To hide the figures, use fig.show=\"hide\".\nIf you want to add an image that is not produced within Rmarkdown. This can be done by using .\nBUT! A better way to add an image is using include_graphics():\n\nknitr::include_graphics('unsw.jpg')\n\n\n\n\nWelcome to UNSW\n\n\n\n\nThe advantage is obvious, you can align the figure, change it size, add a caption, cross-reference it….. It’s all about chunk options.\nThere are lots of chunk options here Chunk options and package options.\n\n\nGlobal chunk options\nYou may be inclined to use largely the same set of chunk options throughout a document. But it would be a pain to retype those options in every chunk. Thus, you want to set some global chunk options at the top of your document.\nFor example, I might use include=FALSE or at least echo=FALSE globally for a report to a scientific collaborator who wouldn’t want to see all of the code. And I might want something like fig.width=12 and fig.height=6 if I generally want those sizes for my figures.\nI’d set such options by having an initial code chunk like this:\nfig.path='Figs/' makes it so the figure files get placed in the Figs subdirectory. (By default, they are not saved at all.)\nNote: the ending slash in Figs/ is important. If you used fig.path='Figs' then the figures would go in the main directory but with Figs as the initial part of their names.\nThe global chunk options become the defaults for the rest of the document. Then if you want a particular chunk to have a different behavior, for example, to have a different figure height, you’d specify a different option within that chunk. For example:\n\npar(mfrow=c(2,2))\nfor(i in 1:4){\n  plot(x[(1:25)*i], y[(1:25)*i]) \n}\n\n\n\n\nMultiple plots\n\n\n\n\nIn a report to a collaborator, I might use include=FALSE, echo=FALSE as a global option, and then use include=TRUE for the chunks that produce figures. Then the code would be suppressed throughout, and any output would be suppressed except in the figure chunks (where I used include=TRUE), which would produce just the figures.\n\n\nPackage options\nIn addition to the chunk options, there are also package options, set with something like:\n\nprogress: (TRUE; logical): Whether to display a progress bar when running knitr::knit().\nverbose: (FALSE; logical): Whether to show verbose information (e.g., R code in each chunk and message logs), or only show chunk labels and options.\n\nPersonally, I don’t usually set package options.\n\n\nReproducible!\nA key motivation for knitr is reproducible research: that our results are accompanied by the data and code needed to produce them. Thus, your report should never explicitly include numbers that are derived from the data. Don’t write “There are 500 simulations.” Rather, insert a bit of code that, when evaluated, gives the number of individuals.\nThat’s the point of the in-line code. You’d write something like this: There are 100 individuals.\nAnother example: The estimated correlation between x and y was 0.8786993. In R Markdown, in-line code is indicated with r. The bit of R code between them is evaluated and the result inserted.\nAn important point: you need to be sure that these in-line bits of code aren’t split across lines in your document. Otherwise you’ll just see the raw code and not the result that you want.\n\n\n\nCross-referencing within documents\nTo use cross-references, you will need:\n\nA bookdown output format: Cross-referencing is not provided directly within the base rmarkdown package, but is provided as an extension in bookdown. We must therefore use an output format from bookdown (e.g., html_document2, pdf_document2, and word_document2, etc.) in the YAML output field.\nA caption to your figure (or table): Figures without a caption will be included directly as images and will therefore not be a numbered figure.\nA labeled code chunk: This provides the identifier for referencing the figure generated by the chunk.\n\nLet’s try to change the output to bookdown::html_document2: default and bookdown::pdf_document2: default. After these conditions are met, we can make cross-references within the text using the syntax @ref(type:label), where label is the chunk label and type is the environment being referenced (e.g. tab, fig, or eqn).\nSee examples\n\nFigure @ref(fig:scatterplot) and Figure @ref(fig:a-taller-figure).\nTable @ref(tab:mtcars).\n\n\n\n\nThe mtcars data.\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n\n\n\n\n\n\nEquation @ref(eq:mean)\n\n\\begin{equation}\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} (\\#eq:mean)\n\\end{equation}\nWe demonstrate a few more math equation environments below. Here is an unnumbered equation using the equation* environment (see this equation is not numbered):\n\\begin{equation*}\n\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x)\n\\end{equation*}\nBelow is an align environment, which allows you to align equal sign (or other signs you want):\n\\begin{align}\ng(X_{n}) &= g(\\theta)+g'({\\tilde{\\theta}})(X_{n}-\\theta) \\notag \\\\\n\\sqrt{n}[g(X_{n})-g(\\theta)] &= g'\\left({\\tilde{\\theta}}\\right)\n  \\sqrt{n}[X_{n}-\\theta ] (\\#eq:align)\n\\end{align}\nYou can use the split environment inside equation so that all lines share the same number @ref(eq:var-beta). By default, each line in the align environment will be assigned an equation number. We suppressed the number of the first line in the previous example using \\notag. In this example, the whole split environment was assigned a single number:\n\\begin{equation}\n\\begin{split}\n\\mathrm{Var}(\\hat{\\beta}) & =\\mathrm{Var}((X'X)^{-1}X'y)\\\\\n& =(X'X)^{-1}X'\\mathrm{Var}(y)((X'X)^{-1}X')'\\\\\n& =(X'X)^{-1}X'\\mathrm{Var}(y)X(X'X)^{-1}\\\\\n& =(X'X)^{-1}X'\\sigma^{2}IX(X'X)^{-1}\\\\\n& =(X'X)^{-1}\\sigma^{2}\n\\end{split}\n(\\#eq:var-beta)\n\\end{equation}\nYou can even reference a section in your report.\n\n\nyour amazing header\nThen I can refer to Section @ref(label-section).\n\n\nCitations\nThere are 5 steps to cite literature and create a Reference section at the end of this file. Here is a good resource if you want to learn other ways of references. (https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#Inline_References)\n\nCreating a new bibliography file: Create a “.bib” file in the same path with this Rmd file. I use TexMaker to create an empty file called “refWeek1.bib”. You can also create a “.bib” file in R Studio. Click “File” -&gt; “New file” -&gt; “R Script”. Then save it as “refWeek1.bib”.\nFill in this file: Go to (https://scholar.google.com/) and search the literature. e.g. “Handbook of data visualization”. Click Citation, then choose BibTex. Copy the citation details and paste it to “refWeek1.bib”. Click Save. Now “refWeek1.bib” is completed and we need to import this file in R Markdown. (If you want cite something without online BibTex source, you can write it yourself in “refWeek1.bib”)\nSpecifying a bibliography: Go back to YAML header (top) of this Rmd file. Type “bibliography: refWeek1.bib”. This command is used to import “refWeek1.bib” we just created.\nInline references: To cite the literature we need use command “@name”. The “name” is the words before the first comma in “refWeek1.bib”, which is “chen2007handbook”. Normally, we don’t change the name. Chen, Härdle, and Unwin (2007)\nBibliography placement: If you want a reference list at the end of this Rmd file, you only need to type “# Reference”.\n\n\n\n\n\n\n\nTask\n\n\n\nCite the book “An Introduction to Statistical Learning.”\nYou are supposed to cite it here. (Hint: you can start from step 2)\n\n\n\n\nComment\nYou can add the command if you want to comment any text in the Rmarkdown using the commands. (Check the command in the Rmd file)\n\n\n\nVisual Markdown Editor\nThe visual editor is friendly to those who are not familiar with Markdown yet, or do not prefer writing Markdown code (see R Markdown Cookbook) . It was experimental in earlier versions of RStudio, and starting with RStudio v2022.02, you can switch between the source mode and the visual mode for a markdown document by simply clicking the “Visual” or “Source” button1 located on the left side of the editor toolbar (see the blog post). To learn more, you can refer to RStudio’s introduction to Visual R Markdown (see https://rstudio.github.io/visual-markdown-editing/).\n\n\n\n\n\n\nTask\n\n\n\nAs an exercise, you can experiment with the following features of the visual editor:\n\nFormat: bold, italic, underline, code, superscript, subscript, etc.\nHeaders: 1st-level header, 2nd-level header… 6th-level header\nHyperlinks\nLists (bulleted list or numbered list)\nInsert: citation, cross-reference (figures, tables, equations), footnote, in-line comment.\nAnd so on.\n\nIn general, this visual editor makes document editing just like in Word. However, it sometimes conflicts with other packages (from my personal experience - in YAML header or LaTeX equations; and there are other causes of problems or limitations, as this editor is still under development!), so you must be very careful when using this editor.\n\nReference",
    "crumbs": [
      "Chapter 1 - Introduction",
      "Lab: R Markdown Introduction"
    ]
  },
  {
    "objectID": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#reference",
    "href": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#reference",
    "title": "Lab: R Markdown Introduction",
    "section": "Reference",
    "text": "Reference",
    "crumbs": [
      "Chapter 1 - Introduction",
      "Lab: R Markdown Introduction"
    ]
  },
  {
    "objectID": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#footnotes",
    "href": "Labs/Ch1-RMarkdown/R_Markdown_Introduction.html#footnotes",
    "title": "Lab: R Markdown Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you don’t see this, please update RStudio to the latest version.↩︎",
    "crumbs": [
      "Chapter 1 - Introduction",
      "Lab: R Markdown Introduction"
    ]
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity-getting-to-know-you",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity-getting-to-know-you",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Activity: Getting to know you",
    "text": "Activity: Getting to know you\n\nSelf introduction\nWhich industry are you working in (plan to work in)?\nWhat do you want to get most out of this course?"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#course-introduction",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#course-introduction",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Course Introduction",
    "text": "Course Introduction\n\nMoodle introduction\nLearning recommendations\nRelationship with ACTL3142/5110 and other courses –&gt;"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#anticipated-achievements",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#anticipated-achievements",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Anticipated achievements",
    "text": "Anticipated achievements\n\nData Science Actuary with problem-solving skills\n\nAn end-to-end process to solve industry challenges\nEnhanced understanding and application of data science techniques in a real-world context\nCommunication skills\nWorking in groups"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#reading-list-for-this-lecture",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#reading-list-for-this-lecture",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Reading List for this lecture",
    "text": "Reading List for this lecture\n\nThe Art of Data Science, Chapters 1, 2 and 3."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#learning-objectives-for-this-lecture",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#learning-objectives-for-this-lecture",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Learning Objectives for this lecture",
    "text": "Learning Objectives for this lecture\n\nUnderstand the process to do data analysis, which is a specific application of the Actuarial Control Cycle\nExplain the key iterative steps involved in a data analytics project\nUnderstand and explain the different types of questions\n\nApply the process to do data analysis in a business setting"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#aims-of-this-course",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#aims-of-this-course",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Aims of this course",
    "text": "Aims of this course\n\n“Data Science Principles – aims to extend students’ knowledge of modern analytical tools and techniques beyond those introduced in the Foundation Program subjects and to teach students how to apply this knowledge in real-life business settings”\n\n\n\n— Actuaries Institute"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#from-data-to-value",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#from-data-to-value",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "From Data to Value",
    "text": "From Data to Value\n\nWhy choose this course?\n\nWith the development of internet, databases, internet of things (IOT) etc., data is everywhere.\nData can be very valuable but it is only valuable if you can analyse and utilize it properly.\n“Insurance is about using statistics to price risk, which is why data, properly collected and used, can transform the core of the product” – Lemonade CEO Daniel Schreiber\nActuarial Data Analytics: integrating data science to actuarial studies.\n\nThe goal of data analytics: ‘Data’ ==&gt; ‘Value’\nThis course will cover the whole process of data anlaytics and its actuarial/business applications"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#interdisciplinary-area",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#interdisciplinary-area",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Interdisciplinary Area",
    "text": "Interdisciplinary Area\n\nAn interdisciplinary covering many different kinds of knowledge.\n\nStatistics\nMachine Learning\nDatabase\nOptimization\nAlgorithm and Programming\nDomain knowledge\n…\n\nThis course mainly focuses on statistical machine learning and its actuarial applications."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#data-analysis-is-an-art",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#data-analysis-is-an-art",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Data Analysis is an Art",
    "text": "Data Analysis is an Art\n\nData analysis is an art. It is not something yet that we can teach to a computer.\nData analysts have many tools at their disposal, from linear regression to classification trees and even deep learning, and these tools have all been carefully taught to computers.\nBut ultimately, a data analyst must find a way to assemble all of the tools and apply them to data to answer a relevant question—a question of interest to people."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#the-actuarial-control-cycle",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#the-actuarial-control-cycle",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "The Actuarial Control Cycle",
    "text": "The Actuarial Control Cycle\n\n\n\nActuarial Control Cycle1\n\n\nReference: Understanding Actuarial Management: the actuarial control cycle"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#data-analysis-cycle",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#data-analysis-cycle",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Data Analysis Cycle",
    "text": "Data Analysis Cycle\n\nData analysis is a highly iterative and non-linear process, better reflected by a series of epicycles.\nThe data analysis process is a specific application of the Actuarial Control Cycle and we call it Data Analysis Cycle (DAC).\n\n\nSource: The Art of Data Science"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#the-5-steps-of-the-data-analysis-cycle-dac",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#the-5-steps-of-the-data-analysis-cycle-dac",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "The 5 steps of the data analysis cycle (DAC)",
    "text": "The 5 steps of the data analysis cycle (DAC)\n\nStating and refining the question\nExploring the data\nBuilding formal statistical models\nInterpreting the results\nCommunicating the results"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#the-epicycle-for-each-step",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#the-epicycle-for-each-step",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "The epicycle for each step",
    "text": "The epicycle for each step\n\nSetting Expectations,\nCollecting information (data), comparing the data to your expectations, and if the expectations don’t match,\nRevising your expectations or fixing the data so your data and your expectations match."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#applying-the-epicycle-to-stating-and-refining-your-question-step-1",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#applying-the-epicycle-to-stating-and-refining-your-question-step-1",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Applying the Epicycle to Stating and Refining Your Question (Step 1)",
    "text": "Applying the Epicycle to Stating and Refining Your Question (Step 1)\nYou can now use the information about the types of questions and characteristics of good questions as a guide to refining your question. To accomplish this, you can iterate through the 3 steps of:\n\nEstablishing your expectations about the question\nGathering information about your question\nDetermining if your expectations match the information you gathered, and then refining your question (or expectations) if your expectations did not match the information you gathered"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Activity",
    "text": "Activity\n\nPlease form into groups and get to know each other.\nPlease select one data science problem (based on your own experience) for your group.\nPlease follow the epicycle analysis to state and refine your question (Step 1 of the Data Analysis Cycle).\nPlease summarise the key points and share them on the Teams channel."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#questions",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#questions",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Questions1:",
    "text": "Questions1:\n\nDescriptive: summarize the measurements in a single data set without further interpretation.\n\nExample: Australian Census\n\nExploratory: builds on a descriptive analysis by searching for discoveries, trends, correlations, or relationships between the measurements to generate ideas or hypotheses.\n\nExample: the four-star planetary system Tatooine was discovered when amateur astronomers explored public astronomical data from the Kepler telescope.\n\nInferential: quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature\n\nExample: a study of whether air pollution correlates with life expectancy at the state level in the United States\n\n\nReference: Leek and Peng (2015)"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#questions-continued",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#questions-continued",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Questions (continued):",
    "text": "Questions (continued):\n\nPredictive: uses a subset of measurements (the features) to predict another measurement (the outcome) on a single person or unit.\n\nExample: web sites use polling data to predict how people will vote in an election.\n\nCausal: asks about whether changing one factor will change another factor, on average, in a population.\n\nExample: decades of data show a clear causal relationship between smoking and cancer.\n\nMechanistic seeks to show that changing one measurement always and exclusively leads to a specific, deterministic behavior in another.\n\nExample: data analysis has shown how wing design changes air flow over a wing, leading to decreased drag. Outside of engineering, mechanistic data analysis is extremely challenging and rarely achievable."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#what-is-the-question",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#what-is-the-question",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "What is the Question?",
    "text": "What is the Question?\n\n“We have found that the most frequent failure in data analysis is mistaking the type of question being considered.”\n\n\n\n\nWhat is the question?1\n\n\nReference: Leek and Peng (2015)"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#example",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#example",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Example",
    "text": "Example\n\nIs it causal?\n\nExample: analyses of the relationship between cellphones and brain cancer (link)"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity-1",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity-1",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Activity",
    "text": "Activity\n\nPlease specify the question type your team has specified in the previous activity\nPlesae share the refined question and the question type by replying to your previous post in the Teams channel.\nPlease challenge your questions:\n\nWhy is this an important question? Who cares about this question?\nHas this question been already answered?\nIs this question answerable?\nIs this question too broad? Can we make it more specific?\n\nWhat makes a good question?"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#characteristics-of-a-good-question",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#characteristics-of-a-good-question",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Characteristics of a Good Question",
    "text": "Characteristics of a Good Question\n\nThe question should be of interest to your audience, the identity of which will depend on the context and environment in which you are working with data\nYou should also check that the question has not already been answered.\nThe question should also stem from a plausible framework\nThe question, should also, of course, be answerable.\nSpecificity is also an important characteristic of a good question."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#translating-a-question-into-a-data-problem",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#translating-a-question-into-a-data-problem",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Translating a Question into a Data Problem1",
    "text": "Translating a Question into a Data Problem1\n\nEvery question must be operationalized as a data analysis that leads to a result.\nThink through what the results of the data analysis would look like and how they might be interpreted.\nWhat sorts of questions don’t lead to interpretable answers?\n\nA question that uses inappropriate data – make sure that the data available to answer your question provide reasonably specific measures of the factors required to answer your question,\na factor that you were not necessarily considering in your question is related to both your exposure of interest and your outcome of interest,\nthe result is not interpretable because the underlying way in which the data were collected lead to a biased result,\nselection bias, which occurs when the data your are analyzing were collected in such a way to inflate the proportion of people who have both characteristics above what exists in the general population.\n\n\nRead Chapter 3.4 of The Art of Data Science for more detailed explanation"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#business-environment",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#business-environment",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Business Environment",
    "text": "Business Environment\n\nObjectives\nConstraints\nOperating environment\n\ncompetitors\nconsumers\nmanagement issues"
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity-2",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.slides.html#activity-2",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Activity",
    "text": "Activity\nAssume you are an actuary working for a consulting firm. In your company, there are now two teams (Team A and Team B) working on different projects.\nTeam A works on forecasting the demand for a grocery retailer. This project needs to estimate the demand so the rest of the supply chain knows the amount of products to send to stores. Once the demand is predicted, the supply chain can work out how much to order, when and where to send it. Over-forecasting leads to waste. Under-forecasting leads to lost sales. Prediction accuracy is the most important consideration for this project.\nTeam B works on predicting the claims for a motor insurer, specifically for their comprehensive insurance product. This project needs to predict the number of claims and the amount of claims next year so as to set appropriate premiums for prospective policy holders. Over-estimating claims will make their premiums non-competitive and under-estimating claims could lead to losses. The interpretability and ease of implementation of the model is more important than prediction accuracy. However, the model should be reasonably predictive.\n\nQuestion: List the Objectives, Operation Environment, and Constratins for each of the cases."
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.html",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.html",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "",
    "text": "Dr. Fei Huang\nLecturer-in-charge\nSchool of Risk and Actuarial Studies\nE: feihuang@unsw.edu.au",
    "crumbs": [
      "Chapter 1 - Introduction",
      "1.1 Introduction to Actuarial Data Science"
    ]
  },
  {
    "objectID": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.html#footnotes",
    "href": "Lectures/Ch1-Introduction/Introduction to Actuarial Data Science.html#footnotes",
    "title": "1.1 Introduction to Actuarial Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReference: Understanding Actuarial Management: the actuarial control cycle↩︎\nReference: Leek and Peng (2015)↩︎\nReference: Leek and Peng (2015)↩︎\nRead Chapter 3.4 of The Art of Data Science for more detailed explanation↩︎",
    "crumbs": [
      "Chapter 1 - Introduction",
      "1.1 Introduction to Actuarial Data Science"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Actuarial Data Science",
    "section": "",
    "text": "This is an online textbook for Actuarial Data Science. It covers an end-to-end problem solving process with data science techniques to tackle various data problems in a business context.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Actuarial Data Science",
    "section": "",
    "text": "This is an online textbook for Actuarial Data Science. It covers an end-to-end problem solving process with data science techniques to tackle various data problems in a business context.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Actuarial Data Science",
    "section": "About Me",
    "text": "About Me\n\nDr. Fei Huang\nSchool of Risk and Actuarial Studies\nE: feihuang@unsw.edu.au",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#chapters",
    "href": "index.html#chapters",
    "title": "Actuarial Data Science",
    "section": "Chapters",
    "text": "Chapters\n\nChapter 1: Introduction\n\n1.1: Introduction to Actuarial Data Science (slides)\n\nChapter 2: Exploratory Data Analysis (EDA)\n\n2.1: Data Visualisation (slides) \n2.2: Data Manipulation and Transformation (slides) \n2.3: Importing, Quality Check and Cleansing (slides)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Actuarial Data Science",
    "section": "Labs",
    "text": "Labs\n\nLab: R Markdown Introduction (pdf) \nLab: Data Visualisation (pdf) \nLab: Data Manipulation and Cleaning (pdf)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#acknowlegement",
    "href": "index.html#acknowlegement",
    "title": "Actuarial Data Science",
    "section": "Acknowlegement",
    "text": "Acknowlegement\n\nThis online textbook is developed for teaching the course “Actuarial Data Science Applications” (coded as ACTL4305 and ACTL5305) at UNSW Sydney.\nI would like to express my gratitude to my tutors of the courses ACTL4305/5305, Yumo Dong, Xi Xin, and Salvatory Kessy, for their invaluable contributions in developing the lab materials.\nA special thanks to all the students who have taken this course from 2020 to the present, whose feedback and engagement have greatly enhanced the course over the years.\nI would also like to extend my appreciation to Dr. Patrick Laub for his assistance in developing the course website using Quarto.\nThe development of this course content has been informed by several key references, including but not limited to the following book references, as well as other sources noted throughout the course materials:\n\nWickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\nPeng, R. D., & Matsui, E. (2016). The Art of Data Science: A guide for anyone who works with Data. Skybrude consulting LLC.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning. New York: Springer (the Second Edition of this book published in 2021).\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media.\n\nLicense\nThis repository includes references from other open books, each subject to their respective licenses. All materials created by me are licensed under the Attribution-NonCommercial 4.0 International license. For more details, please refer to the LICENSE file included in this repository.\nHow to cite\nIf you are using this textbook in your academic work, please find below an example for referencing it using the APA citation style.\nHuang, F. (2024). Actuarial Data Science Online Textbook. GitHub repository. https://feihuangfh.github.io/actuarial_data_science_course/docs/ or https://github.com/feihuangFH/actuarial_data_science_course",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Labs/Ch2-1-DataVisualisation/Data_Visualisation.html",
    "href": "Labs/Ch2-1-DataVisualisation/Data_Visualisation.html",
    "title": "Lab: Data Visualisation",
    "section": "",
    "text": "Learn how to use tidyverse and ggplot() for data visualization.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Visualisation"
    ]
  },
  {
    "objectID": "Labs/Ch2-1-DataVisualisation/Data_Visualisation.html#learning-objectives",
    "href": "Labs/Ch2-1-DataVisualisation/Data_Visualisation.html#learning-objectives",
    "title": "Lab: Data Visualisation",
    "section": "",
    "text": "Learn how to use tidyverse and ggplot() for data visualization.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Visualisation"
    ]
  },
  {
    "objectID": "Labs/Ch2-1-DataVisualisation/Data_Visualisation.html#data-visualization",
    "href": "Labs/Ch2-1-DataVisualisation/Data_Visualisation.html#data-visualization",
    "title": "Lab: Data Visualisation",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nData Source\nWhile it might be difficult to obtain data to address a specific research problem or answer a business question, it is relatively easy to obtain data to test a model or an algorithm for data analysis. In the modern era, readers can obtain datasets from the Internet. The following is a list of some websites to obtain real-world data:\n\nUCI Machine Learning Repository. This website (url: http://archive.ics.uci.edu/ml/index.php) maintains more than 400 datasets that can be used to test machine learning algorithms.\nKaggle. The Kaggle website (url: https://www.kaggle.com/) include real-world datasets used for data science competitions. Readers can download data from Kaggle by registering an account.\nDrivenData. DrivenData aims at bringing cutting-edge practices in data science to solve some of the world’s biggest social challenges. In its website (url: https://www.drivendata.org/), readers can participate in data science competitions and download datasets.\nAnalytics Vidhya. This website (url: https://datahack.analyticsvidhya.com/contest/all/) allows you to participate and download datasets from practice problems and hackathon problems.\nKDD Cup. KDD Cup is the annual Data Mining and Knowledge Discovery competition organized by the ACM Special Interest Group on Knowledge Discovery and Data Mining. This website (url: http://www.kdd.org/kdd-cup) contains the datasets used in past KDD Cup competitions since 1997.\nU.S. Government’s open data. This website (url: https://www.data.gov/) contains about 200,000 datasets covering a wide range of areas including climate, education, energy, and finance.\nAWS Public Datasets. In this website (url: https://aws.amazon.com/datasets/), Amazon provides a centralized repository of public datasets, including some huge datasets.\nCASdatasets: Insurance Datasets. In this website (url: http://cas.uqam.ca/). A collection of datasets, originally for the book ‘Computational Actuarial Science with R’ edited by Arthur Charpentier. Now, the package contains a large variety of actuarial datasets.\n\n\n\nImport Data\nCASdatasets is R friendly, so we can download it by install.packages(\"CASdatasets\", repos = \"http://cas.uqam.ca/pub/\", type=\"source\"). After downloading, when you need this dataset, use library() function.\nBefore you install CASdatasets, make sure you have already installed these packages:\n\ninstall.packages(“zoo”),\ninstall.packages(“xts”),\ninstall.packages(“sp”).\n\n\n#Before you install CASdatasets, make sure you have already installed these packages:\n#install.packages(\"zoo\")\n#install.packages(\"xts\")\n#install.packages(\"sp\")\n#install.packages(\"CASdatasets\", repos = \"http://cas.uqam.ca/pub/\", type=\"source\")\n\nlibrary(CASdatasets)\n#?CASdatasets # Description of this dataset.\n\ndata(freMTPL2freq) # import a dataset we want\ndata(freMTPL2sev) \n\nattach(freMTPL2freq) # The database is attached to the R search path. This means that the database is searched by R when evaluating a variable, so objects in the database can be accessed by simply giving their names.\nattach(freMTPL2sev)\nfreMTPL2freq$ClaimNb &lt;- as.integer(freMTPL2freq$ClaimNb)\n\nNow the datasets called freMTPL2freq and freMTPL2sev are imported successfully. Have a look at this dataset first. In the two datasets freMTPL2freq, freMTPL2sev, risk features are collected for 677,991 motor third-part liability policies (observed mostly on one year). In addition, we have claim numbers by policy as well as the corresponding claim amounts. freMTPL2freq contains the risk features and the claim number while freMTPL2sev contains the claim amount and the corresponding policy ID.\nfreMTPL2freq contains 12 columns:\n\nIDpol: The policy ID (used to link with the claims dataset).\nClaimNb: Number of claims during the exposure period.\nExposure: The period of exposure for a policy, in years.\nArea: The area code.\nVehPower: The power of the car (ordered categorical).\nVehAge: The vehicle age, in years.\nDrivAge: The driver age, in years (in France, people can drive a car at 18).\nBonusMalus: Bonus/malus, between 50 and 350: &lt;100 means bonus, &gt;100 means malus in France.\nVehBrand: The car brand (unknown categories).\nVehGas: The car gas, Diesel or regular.\nDensity: The density of inhabitants (number of inhabitants per km2) in the city the driver of the car lives in.\nRegion: The policy regions in France (based on a standard French classification).\n\nfreMTPL2sev contains 2 columns:\n\nIDpol The occurence date (used to link with the contract dataset).\nClaimAmount The cost of the claim, seen as at a recent date.\n\n\n\nTask 1: How to know the relationship between claim frequency and driver age?\nFirst, we create a figure using the codes from this week’s lecture slide.\n\nlibrary(tidyverse)\n\nggplot(data = freMTPL2freq) + # the dataset\naes(x = DrivAge) + # the x position\naes(y = ClaimNb) +\naes(color = ClaimNb)+\n# the y position\ngeom_point() + # the point geometric shape\n# Adjust axis titles' font size\ntheme(axis.title=element_text(size=14,face=\"bold\"))\n\n\n\n\nDriveAge v.s ClaimNb\n\n\n\n\nIt seems Figure @ref(fig:ageclaimnb) is not informative… WHY?\nIn practice, the frequency of most policy is 0. When you create plots in this way, you can see a bunch of data points at bottom level (0). And the number of observations is large (678013), which makes it difficult to recognize the pattern from so many points…\nWhat I will do is plot the average ClaimNb for each DriveAge. This requires some code you haven’t learned in lecture, but it is really helpful in this case.\n\nfreMTPL2freq %&gt;% \n  group_by(DrivAge) %&gt;% \n  summarize(AveClaimNb=mean(as.double(ClaimNb))) %&gt;% \n  ggplot(aes(x=DrivAge, y=AveClaimNb)) + geom_point() + geom_smooth()\n\n\n\n\nDriveAge v.s Average ClaimNB\n\n\n\n\nFrom Figure @ref(fig:aveclaimnb), now we clearly see a pattern! It tells us younger ages less than 25 and older ages around 80 are more likely to make a claim. So, is this the true story?\nIn general insurance, it’s common to calculate \\text{Claim Rate}_{age}=\\cfrac{\\sum_i\\text{ClaimNb}_{age,i}}{\\sum_i \\text{Exposure}_{age,i}}, where i represents the ith policyholder at this age. Claim Rate tells us the number of claim per exposure (year), which removes the effect of different exposures. Now let’s see the relationship between Claim Rate and DriveAge.\n\nfreMTPL2freq %&gt;% \n  group_by(DrivAge) %&gt;% \n  summarize(claim_rate=sum(as.double(ClaimNb))/sum(Exposure)) %&gt;% \n  ggplot(aes(x=DrivAge, y=claim_rate)) + geom_point() + geom_smooth()\n\n\n\n\nDriveAge v.s Average ClaimRate\n\n\n\n\nFrom Figure @ref(fig:aveclaimrate), we can tell that younger ages become more dangerous while older ages are more safe.\nHere is another informative figure called Violin plot. What do you learn from this plot?\n\n# Violin plot DrivAge vs number of claims:\nfreMTPL2freq %&gt;% filter(ClaimNb&lt;5) %&gt;% \n  mutate(ClaimNb=as.factor(ClaimNb)) %&gt;% \n  ggplot(aes(ClaimNb, DrivAge)) + \n  geom_violin(aes(fill = ClaimNb))\n\n\n\n\nDriveAge v.s Average ClaimRate\n\n\n\n\n\n\nTask 2: How to know the relationship between claim frequency and the area?\nTask 2 is a little bit different with Task 1 because the type of Aera is factor and the type of DrivAge is integer.\n\nstr(freMTPL2freq)\n\n'data.frame':   678013 obs. of  12 variables:\n $ IDpol     : num  1 3 5 10 11 13 15 17 18 21 ...\n $ ClaimNb   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Exposure  : num  0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ...\n $ VehPower  : int  5 5 6 7 7 6 6 7 7 7 ...\n $ VehAge    : int  0 0 2 0 0 2 2 0 0 0 ...\n $ DrivAge   : int  55 55 52 46 46 38 38 33 33 41 ...\n $ BonusMalus: int  50 50 50 50 50 50 50 68 68 50 ...\n $ VehBrand  : Factor w/ 11 levels \"B1\",\"B10\",\"B11\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ VehGas    : chr  \"Regular\" \"Regular\" \"Diesel\" \"Diesel\" ...\n $ Area      : Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 4 4 2 2 2 5 5 3 3 2 ...\n $ Density   : int  1217 1217 54 76 76 3003 3003 137 137 60 ...\n $ Region    : Factor w/ 21 levels \"Alsace\",\"Aquitaine\",..: 21 21 18 2 2 16 16 13 13 17 ...\n\n\nFirst, we try the code in Task 1. Though we can get some information from Figure @ref(fig:crarea), it’s not straightforward.\n\nfreMTPL2freq %&gt;% \n  group_by(Area) %&gt;% \n  summarise(claim_rate=sum(as.double(ClaimNb))/sum(Exposure)) %&gt;% \n  ggplot(aes(x=Area, y=claim_rate)) + geom_point() + geom_smooth()+\n  coord_cartesian(ylim = c(0,0.2)) # set the axis limits\n\n\n\n\nArea v.s Average ClaimRate\n\n\n\n\nA better choice in this case is to use histogram (bar plot) for categorical variable. Now it’s clear to compare the Claim Rates in different areas.\n\nfreMTPL2freq %&gt;% # data piped into\ngroup_by(Area) %&gt;% \nsummarise(claim_rate=sum(as.double(ClaimNb))/sum(Exposure)) %&gt;% \nggplot() + # initiating plot\naes(x = Area) + #categorical variable\naes(y = claim_rate) +\ngeom_col() + #Use `geom_col` to creat a column geometry\naes(color = Area) +\naes(fill = Area) + # new aes 'fill'\naes(linetype = Area)+ #new aes 'linetype'\ncoord_cartesian(ylim = c(0,0.2)) # set the axis limits\n\n\n\n\nArea v.s Average ClaimRate (histogram)",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Visualisation"
    ]
  },
  {
    "objectID": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html",
    "href": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html",
    "title": "Lab: Data Manipulation and Transformation",
    "section": "",
    "text": "Learn how to do data importing, quality check and cleansing.\nLearn how to do data manipulation and transformation.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#learning-objectives",
    "href": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#learning-objectives",
    "title": "Lab: Data Manipulation and Transformation",
    "section": "",
    "text": "Learn how to do data importing, quality check and cleansing.\nLearn how to do data manipulation and transformation.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#case-study-a---french-insurance-dataset",
    "href": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#case-study-a---french-insurance-dataset",
    "title": "Lab: Data Manipulation and Transformation",
    "section": "Case study A - French Insurance Dataset",
    "text": "Case study A - French Insurance Dataset\nWe will continue to use the freMTPL2freq dataset. As a preview, this dataset includes risk features collected for 677,991 motor third-party liability policies, observed mostly over one year. In addtion, freMTPL2freq contains both the risk features and the claim number per policy. The freMTPL2freq dataset consists of 12 columns:\n\nIDpol: The policy ID (used to link with the claims dataset).\nClaimNb: Number of claims during the exposure period.\nExposure: The period of exposure for a policy, in years.\nArea: The area code.\nVehPower: The power of the car (ordered categorical).\nVehAge: The vehicle age, in years.\nDrivAge: The driver age, in years (in France, people can drive a car at 18).\nBonusMalus: Bonus/malus, between 50 and 350: &lt;100 means bonus, &gt;100 means malus in France.\nVehBrand: The car brand (unknown categories).\nVehGas: The car gas, Diesel or regular.\nDensity: The density of inhabitants (number of inhabitants per km2) in the city the driver of the car lives in.\nRegion: The policy regions in France (based on a standard French classification).\n\nLet’s first import the data, and then begin by briefly examining it.\n\n# Load the required packages\nlibrary(CASdatasets)\nlibrary(tidyverse)\n\n# Load the data\ndata(freMTPL2freq)\n\n# Briefly check the data\nstr(freMTPL2freq)\n\n'data.frame':   678013 obs. of  12 variables:\n $ IDpol     : num  1 3 5 10 11 13 15 17 18 21 ...\n $ ClaimNb   : 'table' num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ Exposure  : num  0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ...\n $ VehPower  : int  5 5 6 7 7 6 6 7 7 7 ...\n $ VehAge    : int  0 0 2 0 0 2 2 0 0 0 ...\n $ DrivAge   : int  55 55 52 46 46 38 38 33 33 41 ...\n $ BonusMalus: int  50 50 50 50 50 50 50 68 68 50 ...\n $ VehBrand  : Factor w/ 11 levels \"B1\",\"B10\",\"B11\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ VehGas    : chr  \"Regular\" \"Regular\" \"Diesel\" \"Diesel\" ...\n $ Area      : Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 4 4 2 2 2 5 5 3 3 2 ...\n $ Density   : int  1217 1217 54 76 76 3003 3003 137 137 60 ...\n $ Region    : Factor w/ 21 levels \"Alsace\",\"Aquitaine\",..: 21 21 18 2 2 16 16 13 13 17 ...\n\nsummary(freMTPL2freq)\n\n     IDpol            ClaimNb         Exposure           VehPower     \n Min.   :      1   n.vars :1       Min.   :0.002732   Min.   : 4.000  \n 1st Qu.:1157951   n.cases:36102   1st Qu.:0.180000   1st Qu.: 5.000  \n Median :2272152                   Median :0.490000   Median : 6.000  \n Mean   :2621857                   Mean   :0.528750   Mean   : 6.455  \n 3rd Qu.:4046274                   3rd Qu.:0.990000   3rd Qu.: 7.000  \n Max.   :6114330                   Max.   :2.010000   Max.   :15.000  \n                                                                      \n     VehAge           DrivAge        BonusMalus        VehBrand     \n Min.   :  0.000   Min.   : 18.0   Min.   : 50.00   B12    :166024  \n 1st Qu.:  2.000   1st Qu.: 34.0   1st Qu.: 50.00   B1     :162736  \n Median :  6.000   Median : 44.0   Median : 50.00   B2     :159861  \n Mean   :  7.044   Mean   : 45.5   Mean   : 59.76   B3     : 53395  \n 3rd Qu.: 11.000   3rd Qu.: 55.0   3rd Qu.: 64.00   B5     : 34753  \n Max.   :100.000   Max.   :100.0   Max.   :230.00   B6     : 28548  \n                                                    (Other): 72696  \n    VehGas          Area          Density     \n Length:678013      A:103957   Min.   :    1  \n Class :character   B: 75459   1st Qu.:   92  \n Mode  :character   C:191880   Median :  393  \n                    D:151596   Mean   : 1792  \n                    E:137167   3rd Qu.: 1658  \n                    F: 17954   Max.   :27000  \n                                              \n                         Region      \n Centre                     :160601  \n Rhone-Alpes                : 84752  \n Provence-Alpes-Cotes-D'Azur: 79315  \n Ile-de-France              : 69791  \n Bretagne                   : 42122  \n Nord-Pas-de-Calais         : 40275  \n (Other)                    :201157  \n\n\nFrom the outputs above, we can see that there are 678013 individual car insurance policies and 12 variables associated with each policy. At first glance, without further checking, we notice that the data types of some columns may need adjustment. For example, ClaimNb is stored as a table, and VehGas is stored as a character. We may want to convert these to integer and factor, respectively. However, note that some modeling packages are smart enough to handle this automatically, so we may not need to do this ourselves.\n\n# Load the required packages\n# Convert ClaimNb from a table to integer\nfreMTPL2freq$ClaimNb &lt;- as.integer(as.numeric(freMTPL2freq$ClaimNb))\n\n# Convert VehGas from character to factor\nfreMTPL2freq$VehGas &lt;- as.factor(freMTPL2freq$VehGas)\n\n# Recheck the data structure after adjustment \n# str(freMTPL2freq)\n# summary(freMTPL2freq)\n\n\nTask Solution: Are There Any NA (Missing) Values Present in the Dataset?\n\n# Check for NA values in freMTPL2freq\nna_summary_freq &lt;- sapply(freMTPL2freq, function(x) sum(is.na(x)))\nprint(na_summary_freq)\n\n     IDpol    ClaimNb   Exposure   VehPower     VehAge    DrivAge BonusMalus \n         0          0          0          0          0          0          0 \n  VehBrand     VehGas       Area    Density     Region \n         0          0          0          0          0 \n\n\nFortunately, there are no missing values in this dataset.\n\n\nTask Solution: Check the Distribution of Claim Exposure and Number of Claims, and Comment on Any Unusual Observations\n\n# Histogram of claim exposure using ggplot2\nggplot(freMTPL2freq, aes(x = Exposure)) +\n  geom_histogram(binwidth = 0.1, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Claim Exposure\", x = \"Exposure\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Density plot of claim exposure using ggplot2\nggplot(freMTPL2freq, aes(x = Exposure)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  labs(title = \"Density Plot of Claim Exposure\", x = \"Exposure\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Boxplot of claim exposure using ggplot2\nggplot(freMTPL2freq, aes(y = Exposure)) +\n  geom_boxplot(fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Boxplot of Claim Exposure\", y = \"Exposure\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Frequency table of the number of claims using dplyr\nfreMTPL2freq %&gt;%\n  count(ClaimNb) %&gt;%\n  print()\n\n   ClaimNb      n\n1        0 643953\n2        1  32178\n3        2   1784\n4        3     82\n5        4      7\n6        5      2\n7        6      1\n8        8      1\n9        9      1\n10      11      3\n11      16      1\n\n\nWe consider several plots to depict the distribution of claim exposure. Typically, you would only need to show one of these if you want to include exposure in your EDA. Note that some exposures are greater than one year (i.e., 1224 policies). Additionally, we present the frequency table of the number of claims. There are only 9 policies with more than 4 claims, as shown in the table. Without further information, it is difficult to determine whether these entries are errors or not. You can choose to keep them or consider capping them (e.g., in Noll, Salzmann, and Wuthrich (2020), all exposures greater than 1 are set to 1, and all claim numbers greater than 4 are set to 4).\n\n\nTask Solution: Check if Area Is an Ordinal Categorical Variable\n\n# Calculate total exposure per area code\ntotal_exposure_per_area &lt;- freMTPL2freq %&gt;%\n  group_by(Area) %&gt;%\n  summarise(TotalExposure = sum(Exposure, na.rm = TRUE))\n\n# Bar plot of total exposure per area code using ggplot2\nggplot(total_exposure_per_area, aes(x = Area, y = TotalExposure)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Total Exposure per Area Code\", x = \"Area Code\", y = \"Total Exposure\") +\n  theme_minimal() \n\n\n\n\n\n\n\n# Calculate claim frequency per area code\nclaim_frequency_per_area &lt;- freMTPL2freq %&gt;%\n  group_by(Area) %&gt;%\n  summarise(TotalClaims = sum(ClaimNb, na.rm = TRUE),\n            TotalExposure = sum(Exposure, na.rm = TRUE),\n            ClaimFrequency = TotalClaims / TotalExposure)\n\n# Bar plot of claim frequency per area code using ggplot2\nggplot(claim_frequency_per_area, aes(x = Area, y = ClaimFrequency)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  labs(title = \"Observed Claim Frequency per Area Code\", x = \"Area Code\", y = \"Claim Frequency\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\nWe first checked whether the level of total exposure is roughly the same for each area, which is not the case; Area F clearly has the lowest total exposure. Then, by examining the observed claim frequency per area code, we confirmed that Area is an ordinal categorical variable, as the observed claim frequency increases consistently from Area A to Area F.\n\n\n\n\n\n\nExercise\n\n\n\nIs VehPower an ordinal variable? Can you follow the code above to check this?\n\n\n\n\nTask Solution: Explore the Relationship Between Age and Claim Frequency. How Does Age Influence the Frequency of Claims?\n\n# Calculate total exposure per driver's age group\ntotal_exposure_per_age &lt;- freMTPL2freq %&gt;%\n  group_by(DrivAge) %&gt;%\n  summarise(TotalExposure = sum(Exposure, na.rm = TRUE)) %&gt;%\n  arrange(DrivAge)\n\n# Bar plot of total exposure per driver's age group using ggplot2\nggplot(total_exposure_per_age, aes(x = DrivAge, y = TotalExposure)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Total Exposure per Driver's Age Group\", x = \"Driver's Age\", y = \"Total Exposure\") +\n  theme_minimal() \n\n\n\n\n\n\n\n# Calculate observed frequency per driver's age group\nobserved_frequency_per_age &lt;- freMTPL2freq %&gt;%\n  group_by(DrivAge) %&gt;%\n  summarise(TotalClaims = sum(ClaimNb, na.rm = TRUE),\n            TotalExposure = sum(Exposure, na.rm = TRUE),\n            ObservedFrequency = TotalClaims / TotalExposure) %&gt;%\n  arrange(DrivAge)\n\n# Line plot of observed frequency per driver's age group using ggplot2\nggplot(observed_frequency_per_age, aes(x = DrivAge, y = ObservedFrequency)) +\n  geom_point(color = \"red\", size = 2) +\n  labs(title = \"Observed Frequency per Driver's Age Group\", x = \"Driver's Age\", y = \"Observed Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the above plots, we can observe that the relationship between the predictor Age and the observed claim frequency is non-linear. Please note this, as we will explore how to incorporate this into modeling in the coming weeks.\n\n\n\n\n\n\nExercise\n\n\n\nCan you follow the code above or write your own code to explore the relationship between the (observed) claim frequency and other predictors in the dataset? Did you find any interesting findings?\n\n\n\n\nTask Solution: Analyze the Interrelationships Between the Various Predictors in the Dataset. Identify Any Significant Correlations or Dependencies, and Discuss Their Potential Implications for Modeling.\n\n# Convert the Area factor to numeric based on its levels\nfreMTPL2freq$AreaNumeric &lt;- as.numeric(as.ordered(freMTPL2freq$Area))\n\n# Select the relevant variables\ncorrelation_data &lt;- freMTPL2freq %&gt;%\n  select(AreaNumeric, VehPower, VehAge, DrivAge, BonusMalus, Density)\n\n# Calculate the Pearson correlation matrix\ncorrelation_matrix &lt;- cor(correlation_data, method = \"pearson\")\n\n# Display the correlation matrix\nprint(correlation_matrix)\n\n             AreaNumeric     VehPower       VehAge      DrivAge  BonusMalus\nAreaNumeric  1.000000000  0.003176694 -0.104530220 -0.045180127  0.12085798\nVehPower     0.003176694  1.000000000 -0.006001487  0.030107579 -0.07589469\nVehAge      -0.104530220 -0.006001487  1.000000000 -0.059213383  0.07992307\nDrivAge     -0.045180127  0.030107579 -0.059213383  1.000000000 -0.47996604\nBonusMalus   0.120857981 -0.075894688  0.079923071 -0.479966037  1.00000000\nDensity      0.589375413  0.042900681 -0.090427830 -0.004699793  0.07771679\n                 Density\nAreaNumeric  0.589375413\nVehPower     0.042900681\nVehAge      -0.090427830\nDrivAge     -0.004699793\nBonusMalus   0.077716791\nDensity      1.000000000\n\n# Load additional packages for visualization if needed\nlibrary(ggcorrplot)\n\n# Visualize the Pearson correlation matrix\nggcorrplot(correlation_matrix, \n           method = \"circle\", \n           type = \"lower\", \n           lab = TRUE, \n           title = \"Pearson Correlation Matrix\")\n\n\n\n\n\n\n\n\nHere, we focus on checking the correlations between numerical and ordinal categorical features. Notably, there is a strong positive correlation between Area and Density, followed by a negative dependence between DrivAge and BonusMalus. Examining relationships between features is important because it helps identify multicollinearity, reveals potential interactions, and provides insights into how features jointly influence the target variable.\n\n\n\n\n\n\nExercise\n\n\n\nIn the above, we only considered Pearson’s correlation between numerical features. Can you explore more of the interrelationships between predictors? For example, we might be interested in how vehicle brand interplays with other vehicle characteristics, or even with driver or policy characteristics.\nFor your reference, you can refer to for some in-depth bivariate analysis in EDA for this dataset.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#case-study-b---default-of-credit-card-clients",
    "href": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#case-study-b---default-of-credit-card-clients",
    "title": "Lab: Data Manipulation and Transformation",
    "section": "Case study B - Default of Credit Card Clients",
    "text": "Case study B - Default of Credit Card Clients\nThe data set is the customers’ default payments which include 30000 instances described over 24 attributes. The data can be downloaded from link. This case study considers the customers default payments in Taiwan and compares the predictive accuracy of probability of default among the shrinkage techniques namely lasso, ridge, and elastic net regression and non-shrinkage methods such as logistic regression. This case study employs a binary variable, default payment (Yes =1, No = 0), as the response variable. The data used in this case study have 23 variables as explanatory variables:\n\nX1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\nX2: Gender (1 = male; 2 = female).\nX3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\nX4: Marital status (1 = married; 2 = single; 3 = others).\nX5: Age (year).\nX6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6= the repayment status in September, 2005; X7 = the repayment status in August, 2005; \\ldots; X11 = the repayment status in April, 2005. The measurement scale1 for the repayment status is: -2: No consumption; -1: Paid in full; 0: The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\nX12- X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; \\ldots ; X17 = amount of bill statement in April, 2005.\nX18 - X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005;\\ldots; X23 = amount paid in April, 2005.\n\n\nImport data\n\nThe credit card issuers in Taiwan faced the cash and credit card debt crisis in 2005. To increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, they overused credit card for consumption and accumulated heavy credit and cash card debts. The crisis caused the blow to consumer finance confidence and it was a big challenge for both banks and cardholders. In a well-developed financial system, crisis management is on the downstream and risk prediction is on the upstream. The major purpose of risk prediction is to use financial information, such as business financial statements, customer transactions, and repayment records to predict business performance or individual customers’ credit risk and to reduce the damage and uncertainty.\nThis tutorial focus on how to pre-process the data before using the machine learning techniques to predict the response variable.\nIn this tutorial, we use the credit data of the credit card clients in Taiwan. The data set is the customers’ default payments which include 30000 instances described over 24 attributes. This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\nLoading the required packages\n\n\nlibrary(data.table)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(naniar)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(gridExtra)\nlibrary(ggcorrplot)\nlibrary(glmnet) \n\n\nImporting data\n\n\ndata &lt;- read_excel(\"credit.xls\", skip = 1)\n\n\nUnderstanding the data structure\n\n\ndim(data) # dimension of data\n\n[1] 30000    25\n\nstr(data) # structure of data\n\ntibble [30,000 × 25] (S3: tbl_df/tbl/data.frame)\n $ ID                        : num [1:30000] 1 2 3 4 5 6 7 8 9 10 ...\n $ LIMIT_BAL                 : num [1:30000] 20000 120000 90000 50000 50000 50000 500000 100000 140000 20000 ...\n $ SEX                       : num [1:30000] 2 2 2 2 1 1 1 2 2 1 ...\n $ EDUCATION                 : num [1:30000] 2 2 2 2 2 1 1 2 3 3 ...\n $ MARRIAGE                  : num [1:30000] 1 2 2 1 1 2 2 2 1 2 ...\n $ AGE                       : num [1:30000] 24 26 34 37 57 37 29 23 28 35 ...\n $ PAY_0                     : num [1:30000] 2 -1 0 0 -1 0 0 0 0 -2 ...\n $ PAY_2                     : num [1:30000] 2 2 0 0 0 0 0 -1 0 -2 ...\n $ PAY_3                     : num [1:30000] -1 0 0 0 -1 0 0 -1 2 -2 ...\n $ PAY_4                     : num [1:30000] -1 0 0 0 0 0 0 0 0 -2 ...\n $ PAY_5                     : num [1:30000] -2 0 0 0 0 0 0 0 0 -1 ...\n $ PAY_6                     : num [1:30000] -2 2 0 0 0 0 0 -1 0 -1 ...\n $ BILL_AMT1                 : num [1:30000] 3913 2682 29239 46990 8617 ...\n $ BILL_AMT2                 : num [1:30000] 3102 1725 14027 48233 5670 ...\n $ BILL_AMT3                 : num [1:30000] 689 2682 13559 49291 35835 ...\n $ BILL_AMT4                 : num [1:30000] 0 3272 14331 28314 20940 ...\n $ BILL_AMT5                 : num [1:30000] 0 3455 14948 28959 19146 ...\n $ BILL_AMT6                 : num [1:30000] 0 3261 15549 29547 19131 ...\n $ PAY_AMT1                  : num [1:30000] 0 0 1518 2000 2000 ...\n $ PAY_AMT2                  : num [1:30000] 689 1000 1500 2019 36681 ...\n $ PAY_AMT3                  : num [1:30000] 0 1000 1000 1200 10000 657 38000 0 432 0 ...\n $ PAY_AMT4                  : num [1:30000] 0 1000 1000 1100 9000 ...\n $ PAY_AMT5                  : num [1:30000] 0 0 1000 1069 689 ...\n $ PAY_AMT6                  : num [1:30000] 0 2000 5000 1000 679 ...\n $ default payment next month: num [1:30000] 1 1 0 0 0 0 0 0 0 0 ...\n\n\n\nRenaming some columns\n\n\ncolnames(data)[colnames(data) == \"PAY_0\"] = \"PAY_1\"\ncolnames(data)[colnames((data)) == \"default payment next month\"] = \"default\"\ndata$default &lt;- as.factor(data$default) # changes it\ndata$SEX &lt;- as.factor(data$SEX)\n\n\n\nTask Solution: Are there any missing values in the data? If there are any missing values suggest the ways to impute them. Use the suggested method to impute the missing values.\n\nChecking missing values in the data.\n\nvis_miss(data) # 0% of them are N.A.\n\n\n\n\n\n\n\ncolSums(is.na(data))\n\n       ID LIMIT_BAL       SEX EDUCATION  MARRIAGE       AGE     PAY_1     PAY_2 \n        0         0         0         0         0         0         0         0 \n    PAY_3     PAY_4     PAY_5     PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 \n        0         0         0         0         0         0         0         0 \nBILL_AMT5 BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6 \n        0         0         0         0         0         0         0         0 \n  default \n        0 \n\nsummary(data)\n\n       ID          LIMIT_BAL       SEX         EDUCATION        MARRIAGE    \n Min.   :    1   Min.   :  10000   1:11888   Min.   :0.000   Min.   :0.000  \n 1st Qu.: 7501   1st Qu.:  50000   2:18112   1st Qu.:1.000   1st Qu.:1.000  \n Median :15000   Median : 140000             Median :2.000   Median :2.000  \n Mean   :15000   Mean   : 167484             Mean   :1.853   Mean   :1.552  \n 3rd Qu.:22500   3rd Qu.: 240000             3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :30000   Max.   :1000000             Max.   :6.000   Max.   :3.000  \n      AGE            PAY_1             PAY_2             PAY_3        \n Min.   :21.00   Min.   :-2.0000   Min.   :-2.0000   Min.   :-2.0000  \n 1st Qu.:28.00   1st Qu.:-1.0000   1st Qu.:-1.0000   1st Qu.:-1.0000  \n Median :34.00   Median : 0.0000   Median : 0.0000   Median : 0.0000  \n Mean   :35.49   Mean   :-0.0167   Mean   :-0.1338   Mean   :-0.1662  \n 3rd Qu.:41.00   3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.: 0.0000  \n Max.   :79.00   Max.   : 8.0000   Max.   : 8.0000   Max.   : 8.0000  \n     PAY_4             PAY_5             PAY_6           BILL_AMT1      \n Min.   :-2.0000   Min.   :-2.0000   Min.   :-2.0000   Min.   :-165580  \n 1st Qu.:-1.0000   1st Qu.:-1.0000   1st Qu.:-1.0000   1st Qu.:   3559  \n Median : 0.0000   Median : 0.0000   Median : 0.0000   Median :  22382  \n Mean   :-0.2207   Mean   :-0.2662   Mean   :-0.2911   Mean   :  51223  \n 3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.:  67091  \n Max.   : 8.0000   Max.   : 8.0000   Max.   : 8.0000   Max.   : 964511  \n   BILL_AMT2        BILL_AMT3         BILL_AMT4         BILL_AMT5     \n Min.   :-69777   Min.   :-157264   Min.   :-170000   Min.   :-81334  \n 1st Qu.:  2985   1st Qu.:   2666   1st Qu.:   2327   1st Qu.:  1763  \n Median : 21200   Median :  20088   Median :  19052   Median : 18104  \n Mean   : 49179   Mean   :  47013   Mean   :  43263   Mean   : 40311  \n 3rd Qu.: 64006   3rd Qu.:  60165   3rd Qu.:  54506   3rd Qu.: 50190  \n Max.   :983931   Max.   :1664089   Max.   : 891586   Max.   :927171  \n   BILL_AMT6          PAY_AMT1         PAY_AMT2          PAY_AMT3     \n Min.   :-339603   Min.   :     0   Min.   :      0   Min.   :     0  \n 1st Qu.:   1256   1st Qu.:  1000   1st Qu.:    833   1st Qu.:   390  \n Median :  17071   Median :  2100   Median :   2009   Median :  1800  \n Mean   :  38872   Mean   :  5664   Mean   :   5921   Mean   :  5226  \n 3rd Qu.:  49198   3rd Qu.:  5006   3rd Qu.:   5000   3rd Qu.:  4505  \n Max.   : 961664   Max.   :873552   Max.   :1684259   Max.   :896040  \n    PAY_AMT4         PAY_AMT5           PAY_AMT6        default  \n Min.   :     0   Min.   :     0.0   Min.   :     0.0   0:23364  \n 1st Qu.:   296   1st Qu.:   252.5   1st Qu.:   117.8   1: 6636  \n Median :  1500   Median :  1500.0   Median :  1500.0            \n Mean   :  4826   Mean   :  4799.4   Mean   :  5215.5            \n 3rd Qu.:  4013   3rd Qu.:  4031.5   3rd Qu.:  4000.0            \n Max.   :621000   Max.   :426529.0   Max.   :528666.0            \n\nunique(data%&gt;%select(\"MARRIAGE\"))\n\n\n  \n\n\nunique(data%&gt;%select(\"EDUCATION\"))\n\n\n  \n\n\nlength(data%&gt;%filter(MARRIAGE==0)%&gt;%pull(\"MARRIAGE\"))\n\n[1] 54\n\nlength(data%&gt;%filter(EDUCATION==0)%&gt;%pull(\"EDUCATION\"))\n\n[1] 14\n\n\n\nNo direct missing values in the data. However, when we look at the summary of the data, there are some missing values in marriage and education named 0.\n\n\n\nPossible ways to impute the missing values.\n\nImpute the missing value in marriage and education by naming the missing values as “others”.\nThe missing values can also be imputed using the mode value.\n\n\n\nImpute the missing values.\n\nmplot1 &lt;- ggplot(data = data, mapping = aes(x = MARRIAGE, fill = default)) +\n  geom_bar() + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Before Imputing\") + stat_count(aes(label = ..count..))\n\n# impute missing values in marriage \n# replace 0s values with 3 (others) \n\ndata$MARRIAGE = ifelse(data%&gt;%select(MARRIAGE) == 0, 3, data$MARRIAGE)\nunique(data%&gt;%select(\"MARRIAGE\"))\n\n\n  \n\n\nmplot2 &lt;-ggplot(data = data, mapping = aes(x = MARRIAGE, fill = default)) +\n  geom_bar() + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"After Imputing\") + stat_count(aes(label = ..count..))\n\ngrid.arrange(mplot1, mplot2, ncol = 2)\n\n\n\n\n\n\n\n# impute missing values in education\n# replace 0s values with 3 (others), and merge 5,and 6 to others. \n\neplot1 &lt;- ggplot(data = data, mapping = aes(x = EDUCATION, fill = default)) +\n  geom_bar() + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Before Imputing\") + stat_count(aes(label = ..count..))\n\ndata$EDUCATION = ifelse(data%&gt;%select(EDUCATION)== 0 |data%&gt;%select(EDUCATION) == 5 \n                        |data%&gt;%select(EDUCATION) == 6, 4, data$EDUCATION) \n# we want to replace 0,5,6 by 4\nunique(data%&gt;%select(\"EDUCATION\"))\n\n\n  \n\n\neplot2 &lt;- ggplot(data = data, mapping = aes(x = EDUCATION, fill = default)) +\n  geom_bar() + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"After Imputing\") + stat_count(aes(label = ..count..))\n\ngrid.arrange(eplot1, eplot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nTask Solution: Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.\n\nExploration of Social Status Predictors\n\n# Checking the number of defaulters \n\npar(mfrow=c(1,3))\n\n# Number of defaulters in marriage\n\ncount &lt;- table(data$MARRIAGE, data$default)/rowSums(table(data$MARRIAGE, data$default))\nbarplot(count[,2], col = \"skyblue4\", main = 'Defaulters on Marriage')\n\n# Number of defaulters in education\n\ncount1 &lt;- table(data$EDUCATION, data$default)/rowSums(table(data$EDUCATION, data$default))\nbarplot(count1[,2], col = \"skyblue4\", main = 'Defaulters on Education')\n\n# Number of defaulters in gender (sex)\n\ncount3 &lt;- table(data$SEX, data$default)/rowSums(table(data$SEX, data$default))\nbarplot(count3[,2], col = \"skyblue4\", main = 'Defaulters on Gender')\n\n\n\n\n\n\n\n\n\nMale persons (male = 1) have more chances to default.\nThe better education the lower chances to default.\nMarried persons have more chances to default.\n\n\n\nExploration of response variable\n\n# proportion of defaulters vs non-defaulters \n\nprop &lt;- prop.table(table(data%&gt;%select(default)))\nbarplot(prop, ylab = \"Prop\", xlab = \"Default\", col = c(\"skyblue4\",\"orange\"),\n        legend = rownames(prop), beside = TRUE)\n\n\n\n\n\n\n\n\n20% at 1, 80% at 0 - Target variable variable is imbalanced. This can be solved by under-sampling, over-sampling or no sampling.\n\n\nExploration of age variable\n\n# box plot for age by default\n\nggplot(data = data, aes(x = as.factor(default), y = AGE, colour = default))+ \n  geom_boxplot(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5))+\n  labs(title='Age by Default', x= 'Default', y ='Age')\n\n\n\n\n\n\n\n# distribution of age \n\nplot1 &lt;- ggplot(data, aes(x = AGE))+ \n  geom_histogram(aes(x = AGE), color =\"blue\", fill=\"skyblue4\") + \n  labs(x =\"Age\",y =\"Counts\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Age Distribution\") \n\nplot2 &lt;- ggplot(data = data, mapping = aes(x = AGE)) + \n  geom_density(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5)) +\n  ggtitle(\"Age Distribution\") +\n  xlab(\"Age\")\n\ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\ndata %&gt;% \n  group_by(AGE) %&gt;% \n  summarize(default_rate=sum(as.double(default)-1)/length(AGE)) %&gt;% \n  ggplot(aes(x=AGE, y=default_rate)) + geom_point() + geom_smooth()\n\n\n\n\n\n\n\n\n\nIn general, we cannot see any obvious patterns in the above plot.\n\n\n\nExploration of balance limit variable\n\nsummary(data%&gt;%select(\"LIMIT_BAL\"))\n\n   LIMIT_BAL      \n Min.   :  10000  \n 1st Qu.:  50000  \n Median : 140000  \n Mean   : 167484  \n 3rd Qu.: 240000  \n Max.   :1000000  \n\n# box plot for limit balance by default\n\nggplot(data = data, aes(x = as.factor(default), y = LIMIT_BAL, colour = default))+ \n  geom_boxplot(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5))+ \n  labs(title='Limit Balance by Default', x= 'Default', y ='Limit Balance')\n\n\n\n\n\n\n\nplot_bal1 &lt;- ggplot(data, aes(x = LIMIT_BAL))+ \n  geom_histogram(aes(x = LIMIT_BAL), color =\"blue\", fill=\"skyblue4\") + \n  labs(x =\"LIMIT_BAL\",y =\"Counts\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Limit Balance Distribution\")\n\n# distribution of limit balance\n\nplot_bal2 &lt;- ggplot(data = data, mapping = aes(x = LIMIT_BAL)) + \n  geom_density(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Limit Balance Distribution\") +\n  xlab(\"LIMIT_BAL\")\n\ngrid.arrange(plot_bal1, plot_bal2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe lower the amount of given credit limit of the balance owing, the bigger the chances to default.\n\n\n\nExploration of amount of bill statement variable\n\nbillamt_colsnames  &lt;- paste0(\"BILL_AMT\", c(1, 2:6))\ndata1 &lt;- data%&gt;%select(starts_with(\"BILL_AMT\"))\nsummary(data1)\n\n   BILL_AMT1         BILL_AMT2        BILL_AMT3         BILL_AMT4      \n Min.   :-165580   Min.   :-69777   Min.   :-157264   Min.   :-170000  \n 1st Qu.:   3559   1st Qu.:  2985   1st Qu.:   2666   1st Qu.:   2327  \n Median :  22382   Median : 21200   Median :  20088   Median :  19052  \n Mean   :  51223   Mean   : 49179   Mean   :  47013   Mean   :  43263  \n 3rd Qu.:  67091   3rd Qu.: 64006   3rd Qu.:  60165   3rd Qu.:  54506  \n Max.   : 964511   Max.   :983931   Max.   :1664089   Max.   : 891586  \n   BILL_AMT5        BILL_AMT6      \n Min.   :-81334   Min.   :-339603  \n 1st Qu.:  1763   1st Qu.:   1256  \n Median : 18104   Median :  17071  \n Mean   : 40311   Mean   :  38872  \n 3rd Qu.: 50190   3rd Qu.:  49198  \n Max.   :927171   Max.   : 961664  \n\n# box plot of the bill amount \n\nplot &lt;- lapply(1:ncol(data1), function(x) ggplot(data = data, \n       mapping = aes(x = default, y = data1[[x]], colour = default)) +\n      geom_boxplot() + theme_minimal() +  labs(y = \"Amount\", x = \"Default\"))\ndo.call(grid.arrange, c(plot, ncol = 2, nrow = 3))\n\n\n\n\n\n\n\n# histogram of the bill amount \n\nplot &lt;- lapply(1:ncol(data1), function(x) ggplot(data = data1, mapping = aes(x = data1[[x]])) + \n  geom_histogram(fill = \"skyblue4\") + theme_minimal() + xlab(paste0(billamt_colsnames[x])) + \n          labs(y = \"Amount\"))\ndo.call(grid.arrange, c(plot, ncol = 2, nrow = 3))\n\n\n\n\n\n\n\n\n\nIn general, we can observe a decreasing trend in the key statistics in the summary table from BILL_AMT1 to BILL_AMT6.\n\n\n\nExploration of history of past payment variable\n\npayamt_colsnames  &lt;- paste0(\"PAY_AMT\", c(1, 2:6))\ndata2 &lt;- data%&gt;%select(starts_with(\"PAY_AMT\"))\n\n# bar plot of history of past payment\n\npay_colsnames  &lt;- paste0(\"PAY_\", c(1, 2:6))\ndata3 &lt;- data%&gt;%select(pay_colsnames)\nplot &lt;- lapply(1:ncol(data3), function(x) \n  ggplot(data = data3, mapping = aes(x = data3[[x]])) + \n  geom_bar(stat = \"count\",fill = \"skyblue4\") + theme_minimal() + \n  xlab(paste0(\"Repayment status\", sep=\" \", pay_colsnames[x])) + xlim(-3,8))\ndo.call(grid.arrange, c(plot, ncol=2, nrow=3))\n\n\n\n\n\n\n\n# number of defaulters in history of past payment\n\npar(mfrow = c(3,2))\ncount4 &lt;-  lapply(1:ncol(data3), function(x) table(data3[[x]], data$default)/rowSums(table(data3[[x]], data$default)))\nplots &lt;- lapply(1:ncol(data3), function(x) barplot(count4[[x]][,2], ylim = c(0, 1), col = \"skyblue4\", main = paste0(\"Defaulters on\", sep=\" \", pay_colsnames[x])))\n\n\n\n\n\n\n\n\n\nHaving a delay, even for 1 month in any of the previous months, increases the chance of default.\n\n\n\n\nTask Solution: Are there any relevant transformations of one or more predictors that might improve the classification model?\n\nRelevant transformations of predictors\n\n# log-transform of age \n\nplot3 &lt;- ggplot(data, aes(x = log(AGE)))+ \n  geom_histogram(aes(x = log(AGE)), color =\"blue\", fill=\"skyblue4\") + \n  labs(x =\"Age\",y =\"Counts\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Log Transform of Age\")\n\nplot4 &lt;- ggplot(data = data, mapping = aes(x = log(AGE))) + \n  geom_density(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Log Transform of Age\") +\n  xlab(\"Age\")\n\ngrid.arrange(plot3, plot4, ncol = 2)\n\n\n\n\n\n\n\n# square-root transform of age \n\nplot5 &lt;- ggplot(data, aes(x = sqrt(AGE)))+ \n  geom_histogram(aes(x = sqrt(AGE)), color =\"blue\", fill=\"skyblue4\") + \n  labs(x =\"Age\",y =\"Counts\") + theme(plot.title = element_text(hjust = 0.5)) +\n  ggtitle(\"Square Root Transform of Age\")\n\nplot6 &lt;- ggplot(data = data, mapping = aes(x = sqrt(AGE))) + \n  geom_density(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Square Root Transform of Age\") +\n  xlab(\"Age\")\ngrid.arrange(plot5, plot6, ncol = 2)\n\n\n\n\n\n\n\n# square-root transform of limit balance  \n\nplot_bal3 &lt;- ggplot(data, aes(x = sqrt(LIMIT_BAL)))+ \n  geom_histogram(aes(x = sqrt(LIMIT_BAL)), color =\"blue\", fill=\"skyblue4\") + \n  labs(x =\"LIMIT_BAL\",y =\"Counts\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Square Root of Limit_Bal\")\n\nplot_bal4 &lt;- ggplot(data = data, mapping = aes(x = sqrt(LIMIT_BAL))) + \n  geom_density(fill=\"skyblue4\") + theme(plot.title = element_text(hjust = 0.5)) + \n  ggtitle(\"Square Root of Limit_Bal\") +\n  xlab(\"LIMIT_BAL\")\n\ngrid.arrange(plot_bal3, plot_bal4, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nTask Solution: Rename the column “default payment next month” as “default”. Are there strong relationships between the default variable and other numeric variables? How can you handle the highly correlated variables?\n\nRelationships between the default variable and other numeric variables\n\nHere we are checking the correlation of default variable with other numeric variables.\n\n\n# correlation plot \n\npayamt_colsnames &lt;- paste0(\"PAY_\", c(1, 2:6))\ndata$default &lt;- as.numeric(data$default)\ncorrplot(cor(data %&gt;%select(-EDUCATION,-SEX, -MARRIAGE,-ID, -payamt_colsnames)), method = \"circle\")\n\n\n\n\n\n\n\n\n\nWe see a high level of linear correlations between the amount of bill statements in different months.\nIn the case of the multicollinearity, we need to use such techniques as Ridge and Lasso regression and the Principal components method.\nWe can even drop some variables if we need to, but the price of this is unbiasedness of estimates and this is not the best decision.\nPCA - Principal Component Analysis\n\n\n# pca \n\npca.model &lt;- prcomp(data %&gt;%select(-EDUCATION,-SEX, -MARRIAGE,-ID, -payamt_colsnames), \n                    center = TRUE, scale. = TRUE)\nsummary(pca.model)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.4333 1.3235 1.02473 1.00038 0.95589 0.93941 0.93376\nProportion of Variance 0.3947 0.1168 0.07001 0.06672 0.06092 0.05883 0.05813\nCumulative Proportion  0.3947 0.5115 0.58151 0.64823 0.70915 0.76798 0.82611\n                           PC8    PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.88285 0.8521 0.82363 0.51373 0.26648 0.20260 0.15919\nProportion of Variance 0.05196 0.0484 0.04522 0.01759 0.00473 0.00274 0.00169\nCumulative Proportion  0.87807 0.9265 0.97170 0.98929 0.99402 0.99676 0.99845\n                          PC15\nStandard deviation     0.15244\nProportion of Variance 0.00155\nCumulative Proportion  1.00000",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#footnotes",
    "href": "Labs/Ch2-2-DataManipulation/Data Manipulation_and_Transformation_Solution.html#footnotes",
    "title": "Lab: Data Manipulation and Transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe original data set description is inconsistent with the data; updated according to https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset/discussion/34608.↩︎",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "Lab: Data Manipulation and Transformation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html",
    "title": "2.1 Data Visualisation",
    "section": "",
    "text": "The Art of Data Science, Chapter 4.1, 4.2.\nR for Data Science Online Book, Chapters 2, 3",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#exploratory-data-analsis-an-introduction",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#exploratory-data-analsis-an-introduction",
    "title": "2.1 Data Visualisation",
    "section": "Exploratory Data Analsis: an Introduction",
    "text": "Exploratory Data Analsis: an Introduction\n\nExploratory Data Analysis (EDA)\n\nA set of procedures to produce descriptive and graphical summaries of the data\nExplore the data as they are without making assumptions\nTo examine your data and understand relationship among variables\nTo determine if there are any problems with your dataset\nTo determine whether the question you are asking can be answered by the data that you have\nTo develop a sketch of the answer to your question\n\n\n\nThe process of exploratory data analysis\nThe EDA is an iterative cycle. You:\n\nFormulate your question\nSearch for answers by\n\nCollect and Import data\nCheck data quality and Cleansing data\nManipulate and Transform data\nVisualise data\n\nUse what you learn to refine your questions and/or generate new questions\n\n\nData Visualization is arguably the most important tool for EDA.\n\n\n\nExample\n\nWhat pattern can you see from this plot?\n\n\n\n\nPlot 1\n\n\n\n\nExample\n\nWhat pattern can you see from this plot?\n\n\n\n\nPlot 2\n\n\n\n\nData Visualization\n\n‘Visually attractive graphics also gather their power from content and interpretations beyond the immediate display of some numbers. The best graphics are about the useful and important, about life and death, about the universe. Beautiful graphics do not traffic with the trivial.’ — Edward Tufte",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#data-visulization-using-ggplot2",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#data-visulization-using-ggplot2",
    "title": "2.1 Data Visualisation",
    "section": "Data Visulization using ggplot2",
    "text": "Data Visulization using ggplot2\n\nExplore features of data using data visualisation\n\nA statistical graphic maps variables of\n\na dataset to\naesthetic properties of\n\ngeometric objects.\n\nggplot2 is part of tidyverse\nUsing ggplot2 to visualise your data\nA ggplot2 grammar guide\n\n\n\nggplot2\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.4.4     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.4\n✔ tidyr   1.3.0     ✔ stringr 1.5.1\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nreload the package everytime you start a new session.\npackage::function()\n\nggplot2::ggplot()\n\n\n\n\nFirst steps\n\nQuestion: Do cars with big engines use more fuel than cars with small engines?\nData: The mpg data frame in ggplot2 (ggplot2::mpg)\n\nmpg contains observations collected by the US Environmental Protection Agency on 38 models of car, 1999-2008.\nA data frame with 234 rows and 11 variables:\n\nVariables:\n\nmanufacturer: manufacturer name\nmodel: model name\ndispl: engine displacement, in litres\nyear: year of manufacture\ncyl: number of cylinders\ntrans: type of transmission\ndrv: the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd\ncty: city miles per gallon\nhwy: highway miles per gallon\nfl: fuel type\nclass: “type” of car\n\n\n\n\nThe mpg data\n\n\nmpg\n\n\n  \n\n\n\n \n\n\nCreating a plot\n\n\nggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + \n  # the y position\n  geom_point() + # the point geometric shape \n  # Adjust axis titles' front size\n  theme(axis.title=element_text(size=14,face=\"bold\"))",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#declaring-data",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#declaring-data",
    "title": "2.1 Data Visualisation",
    "section": "Declaring Data",
    "text": "Declaring Data\n\nDeclaring data: method 1\n\nggplot(data = mpg)\n\n\n\n\n\n\n\n\n\n\nDeclaring data: method 2\n\nPipe data into ggplot() using the pipe operator: %&gt;%\n\n\nmpg %&gt;% # data piped into\n  ggplot() # initiating plot\n\n\n\n\n\n\n\n\n\n\nExercises\n\nRun ggplot(data = mpg). What do you see?\nHow many rows are in mpg? How many columns?\nWhat does the drv variable describe? Read the help for ?mpg to find out.\nMake a scatterplot of hwy vs cyl.\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#aesthetic-mapping",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#aesthetic-mapping",
    "title": "2.1 Data Visualisation",
    "section": "Aesthetic Mapping",
    "text": "Aesthetic Mapping\n\nAesthetic mappings\n\nAn aesthetic is a visual property of the objects in your plot.\nAesthetics include things like the position, size, the shape, or the color of your points.\nMapping: variables are ‘mapped’ to (represented by) aesthetics.\n\n\n\nA main pool of Aesthetics\n\n\n\nA main pool of aesthetics\n\n\nNote: This figure is from Wilke’s Fundamentals of data Visualization.\n\n\naes() means ‘Ask’\n\naes(): What variables are we asking the aesthetic (color, position, shape, etc.) to represent?\naes(color=gender): ‘Please represent the variable gender for me using different colors.’\n\n\n\nMore aes mappings\n \n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + \n  #the point geometric shape, the above aes are requied and\n  #the below are optional\n  #theme(axis.title=element_text(size=14,face=\"bold\"))+\n  aes(color = class) + # Color for type of car\n  #aes(shape = class) +\n  #ggplot2 will only use six shapes at a time. By default, \n  #additional groups will go unplotted when using 'shape'.\n  aes(size = cty) +  # Size for city miles per gallon\n  aes(alpha = year) # transparency for year of manufacture\n\n\n\n\nPlot\n\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nOther aes for other geometric objects\n\nmpg %&gt;% # data piped into\n  ggplot() + # initiating plot\n  aes(x = class) +  #categorical variable \n  aes(y = hwy) + \n  geom_col() + #Use `geom_col` to creat a column geometry\n  aes(color = class) +\n  aes(fill = class) + # new aes 'fill'\n  aes(linetype = class) #new aes 'linetype'\n\n\n\n\n\n\n\n\n\n\nUnmapped aesthetics\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  #Another geom layer with aesthetics \n  #that don't do representation\n  geom_point(\n    color=\"plum4\",\n    size=8,\n    shape=21\n  ) \n\n\n\nPlot\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nExercises:\n\nLook at the help for geom_text (?geom_text). What are the required aesthetics?\nWhich variables in mpgare categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\nMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?\n\n\n\n\n\nggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + \n  aes(color = cty)+ #color, size and shape\n  # the y position\n  geom_point()  # the point geometric shape",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#facets",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#facets",
    "title": "2.1 Data Visualisation",
    "section": "Facets",
    "text": "Facets\n\nFacets: facet_wrap\n\nfacet your plot by a single variable\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  # ~ followed by a discrete variable\n  facet_wrap(~ class, nrow = 2) \n\n\n\n\n\n\n\n\n\n\nFacets: facet_grid\n\nfacet your plot on the combination of two variables \n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  # two variable names separated by a ~\n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n  #facet_grid(. ~ cyl) #not facet in the rows \n  #facet_grid(drv~.) #not facet in the rows",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#geometric-object",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.html#geometric-object",
    "title": "2.1 Data Visualisation",
    "section": "Geometric Object",
    "text": "Geometric Object\n\nA complete sentence of ggplot\n\ndata + aes + geom\nplots \nNouns: geometric objects\n\ngeom_point()\ngeom_col()\ngeom_line()\ngeom_text()\ngeom_segment()\ngeom_smooth()\ngeom_bar()\netc.\n\nThe conditional mood: geom specific data and aesthetic mapping \n\n\n\nDifferent Geoms\n Compare with \n\nlibrary(grid)\nlibrary(gridExtra)\np1=ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) # geom_point\np2=ggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype=drv), se=FALSE) # geom_smooth\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nAnother Example: boxplot\n\np1=ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot()\np2=ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot() +\n  coord_flip() #switches the x and y axes\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nGoing local with data and aesthetics\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ, y = hwy) +\n  geom_point() +\n  aes(color = class) + \n  #xend and yend are required for geom_segment\n  #like creating a column with a single value\n  aes(xend = 0) +  aes(yend = 0) +\n  #geom_segment() draws a straight line \n  #between points (x, y) and (xend, yend)\n  geom_segment(\n    #geom specific data, using 'sebset' to select data\n    data = subset(mpg, fl==\"p\"), \n    # geom specific (local) aesthetics\n    aes(size = cyl, alpha = cty), \n    color = \"orange\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nPlot\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nAnother Example\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + #global aes\n  geom_point(mapping = aes(color = class)) +  #local aes\n  #local data and aes\n  geom_smooth(data = filter(mpg, class == \"subcompact\"), \n              se = TRUE) #se: standard error\n\n\n\n\n\n\n\n\n\n\nAnnotation 1\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  annotate(geom=\"point\",\n           x=3.1,\n           y=27,\n           color=\"red\")\n\n\n\nPlot 1\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nAnnotation 2\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  annotate(geom=\"point\",\n           x=3.1,\n           y=27,\n           color=\"red\")+\n  annotate(geom=\"text\",\n           x=c(2, 4, 6),\n           y=40,\n           label=\"Hello\",\n           color=\"blue\")\n\n\n\nPlot 2\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nAnnotation 3\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  annotate(geom=\"point\",\n           x=3.1,\n           y=27,\n           color=\"red\")+\n  annotate(geom=\"text\",\n           x=c(2, 4, 6),\n           y=40,\n           label=\"Hello\",\n           color=\"blue\")+\n  annotate(geom=\"curve\",\n           x=2,\n           y=39,\n           xend=3,\n           yend=27.3,\n           color=\"green\",\n           arrow=arrow(angle=20))\n\n\n\nPlot 3\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nAnnotation 4\n\nuse geom_abline, geom_hline, and geom_vline\n\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  geom_abline(slope=5, intercept=3) +\n  geom_hline(yintercept= 30, \n             linetype=\"dotted\", color=\"blue\")+\n  geom_vline(xintercept=c(4,5),\n             linetype=\"dashed\", color=\"red\")\n\n\n\nPlot 4\n\nprint(mpg_plot)\n\n\n\n\n\n\n\n\n\n\nExercises\n\nWhat’s gone wrong with this code? Why are the points not blue? \n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, \n                           y = hwy, color = \"blue\"))\n\n\nWhat happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? Note, you’ll also need to specify x and y.\n\n\n\nInteractive data visualisation (optional)\n\nR package: Shiny\ncan host standalone apps on a webpage\n\nExample: Life Expectancy using data from the \n\ncan embed them in R Markdown documents or build dashboards.",
    "crumbs": [
      "Chapter 2 - Exploratory Data Analysis",
      "2.1 Data Visualisation"
    ]
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#reading-list",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#reading-list",
    "title": "2.1 Data Visualisation",
    "section": "Reading List",
    "text": "Reading List\n\nThe Art of Data Science, Chapter 4.1, 4.2.\nR for Data Science Online Book, Chapters 2, 3"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#learning-objectives-of-exploratory-data-analysis",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#learning-objectives-of-exploratory-data-analysis",
    "title": "2.1 Data Visualisation",
    "section": "Learning Objectives of Exploratory Data Analysis",
    "text": "Learning Objectives of Exploratory Data Analysis\n\nUnderstand how to do exploratory data analysis with the tidyverse package in R\nExplore features of data using data visualisation.\nExplain common features of data, such as categorical variables, missing values, unreliable/non-validated data, outliers and high cardinality features, that may lead to problems.\nApply appropriate methods to deal with common data problems.\nApply a range of techniques to assess data quality.\n\nUse R to manipulate (eg filter, merge, sort, group by, summarise, etc.)\nUse R to import, export and tidy data.\nApply the process to do exploratory data analysis with practical datasets\nApply R Markdown for communication and reproducible analysis"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exploratory-data-analysis-eda",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exploratory-data-analysis-eda",
    "title": "2.1 Data Visualisation",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nA set of procedures to produce descriptive and graphical summaries of the data\nExplore the data as they are without making assumptions\nTo examine your data and understand relationship among variables\nTo determine if there are any problems with your dataset\nTo determine whether the question you are asking can be answered by the data that you have\nTo develop a sketch of the answer to your question"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#the-process-of-exploratory-data-analysis",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#the-process-of-exploratory-data-analysis",
    "title": "2.1 Data Visualisation",
    "section": "The process of exploratory data analysis",
    "text": "The process of exploratory data analysis\nThe EDA is an iterative cycle. You:\n\nFormulate your question\nSearch for answers by\n\nCollect and Import data\nCheck data quality and Cleansing data\nManipulate and Transform data\nVisualise data\n\nUse what you learn to refine your questions and/or generate new questions\n\n\nData Visualization is arguably the most important tool for EDA."
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#example",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#example",
    "title": "2.1 Data Visualisation",
    "section": "Example",
    "text": "Example\n\nWhat pattern can you see from this plot?\n\n\nPlot 1"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#example-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#example-1",
    "title": "2.1 Data Visualisation",
    "section": "Example",
    "text": "Example\n\nWhat pattern can you see from this plot?\n\n\nPlot 2"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#data-visualization",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#data-visualization",
    "title": "2.1 Data Visualisation",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n‘Visually attractive graphics also gather their power from content and interpretations beyond the immediate display of some numbers. The best graphics are about the useful and important, about life and death, about the universe. Beautiful graphics do not traffic with the trivial.’ — Edward Tufte"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#explore-features-of-data-using-data-visualisation",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#explore-features-of-data-using-data-visualisation",
    "title": "2.1 Data Visualisation",
    "section": "Explore features of data using data visualisation",
    "text": "Explore features of data using data visualisation\n\nA statistical graphic maps variables of\n\na dataset to\naesthetic properties of\n\ngeometric objects.\n\nggplot2 is part of tidyverse\nUsing ggplot2 to visualise your data\nA ggplot2 grammar guide"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#ggplot2",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#ggplot2",
    "title": "2.1 Data Visualisation",
    "section": "ggplot2",
    "text": "ggplot2\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nreload the package everytime you start a new session.\npackage::function()\n\nggplot2::ggplot()"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#first-steps",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#first-steps",
    "title": "2.1 Data Visualisation",
    "section": "First steps",
    "text": "First steps\n\nQuestion: Do cars with big engines use more fuel than cars with small engines?\nData: The mpg data frame in ggplot2 (ggplot2::mpg)\n\nmpg contains observations collected by the US Environmental Protection Agency on 38 models of car, 1999-2008.\nA data frame with 234 rows and 11 variables:\n\nVariables:\n\nmanufacturer: manufacturer name\nmodel: model name\ndispl: engine displacement, in litres\nyear: year of manufacture\ncyl: number of cylinders\ntrans: type of transmission\ndrv: the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd\ncty: city miles per gallon\nhwy: highway miles per gallon\nfl: fuel type\nclass: “type” of car"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#the-mpg-data",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#the-mpg-data",
    "title": "2.1 Data Visualisation",
    "section": "The mpg data",
    "text": "The mpg data\n\n\nmpg"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#creating-a-plot",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#creating-a-plot",
    "title": "2.1 Data Visualisation",
    "section": "Creating a plot",
    "text": "Creating a plot\n\n\nggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + \n  # the y position\n  geom_point() + # the point geometric shape \n  # Adjust axis titles' front size\n  theme(axis.title=element_text(size=14,face=\"bold\"))"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#declaring-data-method-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#declaring-data-method-1",
    "title": "2.1 Data Visualisation",
    "section": "Declaring data: method 1",
    "text": "Declaring data: method 1\n\nggplot(data = mpg)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#declaring-data-method-2",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#declaring-data-method-2",
    "title": "2.1 Data Visualisation",
    "section": "Declaring data: method 2",
    "text": "Declaring data: method 2\n\nPipe data into ggplot() using the pipe operator: %&gt;%\n\n\nmpg %&gt;% # data piped into\n  ggplot() # initiating plot"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exercises",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exercises",
    "title": "2.1 Data Visualisation",
    "section": "Exercises",
    "text": "Exercises\n\nRun ggplot(data = mpg). What do you see?\nHow many rows are in mpg? How many columns?\nWhat does the drv variable describe? Read the help for ?mpg to find out.\nMake a scatterplot of hwy vs cyl.\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#aesthetic-mappings",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#aesthetic-mappings",
    "title": "2.1 Data Visualisation",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nAn aesthetic is a visual property of the objects in your plot.\nAesthetics include things like the position, size, the shape, or the color of your points.\nMapping: variables are ‘mapped’ to (represented by) aesthetics."
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#a-main-pool-of-aesthetics",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#a-main-pool-of-aesthetics",
    "title": "2.1 Data Visualisation",
    "section": "A main pool of Aesthetics",
    "text": "A main pool of Aesthetics\n\nA main pool of aestheticsNote: This figure is from Wilke’s Fundamentals of data Visualization."
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#aes-means-ask",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#aes-means-ask",
    "title": "2.1 Data Visualisation",
    "section": "aes() means ‘Ask’",
    "text": "aes() means ‘Ask’\n\naes(): What variables are we asking the aesthetic (color, position, shape, etc.) to represent?\naes(color=gender): ‘Please represent the variable gender for me using different colors.’"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#more-aes-mappings",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#more-aes-mappings",
    "title": "2.1 Data Visualisation",
    "section": "More aes mappings",
    "text": "More aes mappings\n \n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + \n  #the point geometric shape, the above aes are requied and\n  #the below are optional\n  #theme(axis.title=element_text(size=14,face=\"bold\"))+\n  aes(color = class) + # Color for type of car\n  #aes(shape = class) +\n  #ggplot2 will only use six shapes at a time. By default, \n  #additional groups will go unplotted when using 'shape'.\n  aes(size = cty) +  # Size for city miles per gallon\n  aes(alpha = year) # transparency for year of manufacture"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot",
    "title": "2.1 Data Visualisation",
    "section": "Plot",
    "text": "Plot\n\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#other-aes-for-other-geometric-objects",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#other-aes-for-other-geometric-objects",
    "title": "2.1 Data Visualisation",
    "section": "Other aes for other geometric objects",
    "text": "Other aes for other geometric objects\n\nmpg %&gt;% # data piped into\n  ggplot() + # initiating plot\n  aes(x = class) +  #categorical variable \n  aes(y = hwy) + \n  geom_col() + #Use `geom_col` to creat a column geometry\n  aes(color = class) +\n  aes(fill = class) + # new aes 'fill'\n  aes(linetype = class) #new aes 'linetype'"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#unmapped-aesthetics",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#unmapped-aesthetics",
    "title": "2.1 Data Visualisation",
    "section": "Unmapped aesthetics",
    "text": "Unmapped aesthetics\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  #Another geom layer with aesthetics \n  #that don't do representation\n  geom_point(\n    color=\"plum4\",\n    size=8,\n    shape=21\n  )"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-1",
    "title": "2.1 Data Visualisation",
    "section": "Plot",
    "text": "Plot\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exercises-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exercises-1",
    "title": "2.1 Data Visualisation",
    "section": "Exercises:",
    "text": "Exercises:\n\nLook at the help for geom_text (?geom_text). What are the required aesthetics?\nWhich variables in mpgare categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\nMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#section",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#section",
    "title": "2.1 Data Visualisation",
    "section": "",
    "text": "ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + \n  aes(color = cty)+ #color, size and shape\n  # the y position\n  geom_point()  # the point geometric shape"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#facets-facet_wrap",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#facets-facet_wrap",
    "title": "2.1 Data Visualisation",
    "section": "Facets: facet_wrap",
    "text": "Facets: facet_wrap\n\nfacet your plot by a single variable\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  # ~ followed by a discrete variable\n  facet_wrap(~ class, nrow = 2)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#facets-facet_grid",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#facets-facet_grid",
    "title": "2.1 Data Visualisation",
    "section": "Facets: facet_grid",
    "text": "Facets: facet_grid\n\nfacet your plot on the combination of two variables \n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  # two variable names separated by a ~\n  facet_grid(drv ~ cyl)\n\n  #facet_grid(. ~ cyl) #not facet in the rows \n  #facet_grid(drv~.) #not facet in the rows"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#a-complete-sentence-of-ggplot",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#a-complete-sentence-of-ggplot",
    "title": "2.1 Data Visualisation",
    "section": "A complete sentence of ggplot",
    "text": "A complete sentence of ggplot\n\ndata + aes + geom\nplots \nNouns: geometric objects\n\ngeom_point()\ngeom_col()\ngeom_line()\ngeom_text()\ngeom_segment()\ngeom_smooth()\ngeom_bar()\netc.\n\nThe conditional mood: geom specific data and aesthetic mapping"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#different-geoms",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#different-geoms",
    "title": "2.1 Data Visualisation",
    "section": "Different Geoms",
    "text": "Different Geoms\n Compare with \n\nlibrary(grid)\nlibrary(gridExtra)\np1=ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) # geom_point\np2=ggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype=drv), se=FALSE) # geom_smooth\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#another-example-boxplot",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#another-example-boxplot",
    "title": "2.1 Data Visualisation",
    "section": "Another Example: boxplot",
    "text": "Another Example: boxplot\n\np1=ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot()\np2=ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot() +\n  coord_flip() #switches the x and y axes\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#going-local-with-data-and-aesthetics",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#going-local-with-data-and-aesthetics",
    "title": "2.1 Data Visualisation",
    "section": "Going local with data and aesthetics",
    "text": "Going local with data and aesthetics\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ, y = hwy) +\n  geom_point() +\n  aes(color = class) + \n  #xend and yend are required for geom_segment\n  #like creating a column with a single value\n  aes(xend = 0) +  aes(yend = 0) +\n  #geom_segment() draws a straight line \n  #between points (x, y) and (xend, yend)\n  geom_segment(\n    #geom specific data, using 'sebset' to select data\n    data = subset(mpg, fl==\"p\"), \n    # geom specific (local) aesthetics\n    aes(size = cyl, alpha = cty), \n    color = \"orange\"\n  )"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-2",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-2",
    "title": "2.1 Data Visualisation",
    "section": "Plot",
    "text": "Plot\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#another-example",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#another-example",
    "title": "2.1 Data Visualisation",
    "section": "Another Example",
    "text": "Another Example\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + #global aes\n  geom_point(mapping = aes(color = class)) +  #local aes\n  #local data and aes\n  geom_smooth(data = filter(mpg, class == \"subcompact\"), \n              se = TRUE) #se: standard error"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-1",
    "title": "2.1 Data Visualisation",
    "section": "Annotation 1",
    "text": "Annotation 1\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  annotate(geom=\"point\",\n           x=3.1,\n           y=27,\n           color=\"red\")"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-1-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-1-1",
    "title": "2.1 Data Visualisation",
    "section": "Plot 1",
    "text": "Plot 1\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-2",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-2",
    "title": "2.1 Data Visualisation",
    "section": "Annotation 2",
    "text": "Annotation 2\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  annotate(geom=\"point\",\n           x=3.1,\n           y=27,\n           color=\"red\")+\n  annotate(geom=\"text\",\n           x=c(2, 4, 6),\n           y=40,\n           label=\"Hello\",\n           color=\"blue\")"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-2-1",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-2-1",
    "title": "2.1 Data Visualisation",
    "section": "Plot 2",
    "text": "Plot 2\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-3",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-3",
    "title": "2.1 Data Visualisation",
    "section": "Annotation 3",
    "text": "Annotation 3\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  annotate(geom=\"point\",\n           x=3.1,\n           y=27,\n           color=\"red\")+\n  annotate(geom=\"text\",\n           x=c(2, 4, 6),\n           y=40,\n           label=\"Hello\",\n           color=\"blue\")+\n  annotate(geom=\"curve\",\n           x=2,\n           y=39,\n           xend=3,\n           yend=27.3,\n           color=\"green\",\n           arrow=arrow(angle=20))"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-3",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-3",
    "title": "2.1 Data Visualisation",
    "section": "Plot 3",
    "text": "Plot 3\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-4",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#annotation-4",
    "title": "2.1 Data Visualisation",
    "section": "Annotation 4",
    "text": "Annotation 4\n\nuse geom_abline, geom_hline, and geom_vline\n\n\nmpg_plot= ggplot(data = mpg) + # the dataset  \n  aes(x = displ) + # the x position\n  aes(y = hwy) + # the y position\n  geom_point() + # the point geometric shape\n  geom_abline(slope=5, intercept=3) +\n  geom_hline(yintercept= 30, \n             linetype=\"dotted\", color=\"blue\")+\n  geom_vline(xintercept=c(4,5),\n             linetype=\"dashed\", color=\"red\")"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-4",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#plot-4",
    "title": "2.1 Data Visualisation",
    "section": "Plot 4",
    "text": "Plot 4\n\nprint(mpg_plot)"
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exercises-2",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#exercises-2",
    "title": "2.1 Data Visualisation",
    "section": "Exercises",
    "text": "Exercises\n\nWhat’s gone wrong with this code? Why are the points not blue? \n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, \n                           y = hwy, color = \"blue\"))\n\n\nWhat happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? Note, you’ll also need to specify x and y."
  },
  {
    "objectID": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#interactive-data-visualisation-optional",
    "href": "Lectures/Ch2-1-DataVisualisation/Lecture-1-2-Data-Visualisation.slides.html#interactive-data-visualisation-optional",
    "title": "2.1 Data Visualisation",
    "section": "Interactive data visualisation (optional)",
    "text": "Interactive data visualisation (optional)\n\nR package: Shiny\ncan host standalone apps on a webpage\n\nExample: Life Expectancy using data from the \n\ncan embed them in R Markdown documents or build dashboards."
  }
]